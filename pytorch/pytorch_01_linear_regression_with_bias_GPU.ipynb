{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 3201.9794921875\n",
      "epoch 2, loss 3188.662109375\n",
      "epoch 3, loss 3175.400634765625\n",
      "epoch 4, loss 3162.195556640625\n",
      "epoch 5, loss 3149.046142578125\n",
      "epoch 6, loss 3135.95361328125\n",
      "epoch 7, loss 3122.916259765625\n",
      "epoch 8, loss 3109.93408203125\n",
      "epoch 9, loss 3097.006591796875\n",
      "epoch 10, loss 3084.13427734375\n",
      "epoch 11, loss 3071.315673828125\n",
      "epoch 12, loss 3058.552490234375\n",
      "epoch 13, loss 3045.842529296875\n",
      "epoch 14, loss 3033.185302734375\n",
      "epoch 15, loss 3020.583740234375\n",
      "epoch 16, loss 3008.03466796875\n",
      "epoch 17, loss 2995.539306640625\n",
      "epoch 18, loss 2983.095458984375\n",
      "epoch 19, loss 2970.704833984375\n",
      "epoch 20, loss 2958.36572265625\n",
      "epoch 21, loss 2946.080322265625\n",
      "epoch 22, loss 2933.84765625\n",
      "epoch 23, loss 2921.666748046875\n",
      "epoch 24, loss 2909.53515625\n",
      "epoch 25, loss 2897.455810546875\n",
      "epoch 26, loss 2885.42724609375\n",
      "epoch 27, loss 2873.449462890625\n",
      "epoch 28, loss 2861.522216796875\n",
      "epoch 29, loss 2849.6455078125\n",
      "epoch 30, loss 2837.8203125\n",
      "epoch 31, loss 2826.044677734375\n",
      "epoch 32, loss 2814.317626953125\n",
      "epoch 33, loss 2802.6416015625\n",
      "epoch 34, loss 2791.012939453125\n",
      "epoch 35, loss 2779.43408203125\n",
      "epoch 36, loss 2767.9033203125\n",
      "epoch 37, loss 2756.42138671875\n",
      "epoch 38, loss 2744.989013671875\n",
      "epoch 39, loss 2733.603515625\n",
      "epoch 40, loss 2722.2666015625\n",
      "epoch 41, loss 2710.976318359375\n",
      "epoch 42, loss 2699.734375\n",
      "epoch 43, loss 2688.541259765625\n",
      "epoch 44, loss 2677.3935546875\n",
      "epoch 45, loss 2666.2919921875\n",
      "epoch 46, loss 2655.2392578125\n",
      "epoch 47, loss 2644.231201171875\n",
      "epoch 48, loss 2633.271484375\n",
      "epoch 49, loss 2622.356689453125\n",
      "epoch 50, loss 2611.4873046875\n",
      "epoch 51, loss 2600.66455078125\n",
      "epoch 52, loss 2589.886962890625\n",
      "epoch 53, loss 2579.154296875\n",
      "epoch 54, loss 2568.466552734375\n",
      "epoch 55, loss 2557.82421875\n",
      "epoch 56, loss 2547.2255859375\n",
      "epoch 57, loss 2536.673095703125\n",
      "epoch 58, loss 2526.163818359375\n",
      "epoch 59, loss 2515.69921875\n",
      "epoch 60, loss 2505.27978515625\n",
      "epoch 61, loss 2494.9033203125\n",
      "epoch 62, loss 2484.5703125\n",
      "epoch 63, loss 2474.2802734375\n",
      "epoch 64, loss 2464.032958984375\n",
      "epoch 65, loss 2453.829833984375\n",
      "epoch 66, loss 2443.668701171875\n",
      "epoch 67, loss 2433.55078125\n",
      "epoch 68, loss 2423.47607421875\n",
      "epoch 69, loss 2413.442626953125\n",
      "epoch 70, loss 2403.451416015625\n",
      "epoch 71, loss 2393.50244140625\n",
      "epoch 72, loss 2383.59423828125\n",
      "epoch 73, loss 2373.7265625\n",
      "epoch 74, loss 2363.902099609375\n",
      "epoch 75, loss 2354.118896484375\n",
      "epoch 76, loss 2344.376220703125\n",
      "epoch 77, loss 2334.674560546875\n",
      "epoch 78, loss 2325.014404296875\n",
      "epoch 79, loss 2315.39404296875\n",
      "epoch 80, loss 2305.8134765625\n",
      "epoch 81, loss 2296.2734375\n",
      "epoch 82, loss 2286.7734375\n",
      "epoch 83, loss 2277.31298828125\n",
      "epoch 84, loss 2267.892333984375\n",
      "epoch 85, loss 2258.5107421875\n",
      "epoch 86, loss 2249.1689453125\n",
      "epoch 87, loss 2239.865966796875\n",
      "epoch 88, loss 2230.6025390625\n",
      "epoch 89, loss 2221.37744140625\n",
      "epoch 90, loss 2212.1904296875\n",
      "epoch 91, loss 2203.04248046875\n",
      "epoch 92, loss 2193.93212890625\n",
      "epoch 93, loss 2184.859619140625\n",
      "epoch 94, loss 2175.826171875\n",
      "epoch 95, loss 2166.8310546875\n",
      "epoch 96, loss 2157.873046875\n",
      "epoch 97, loss 2148.95166015625\n",
      "epoch 98, loss 2140.067626953125\n",
      "epoch 99, loss 2131.220703125\n",
      "epoch 100, loss 2122.4111328125\n",
      "epoch 101, loss 2113.638427734375\n",
      "epoch 102, loss 2104.9013671875\n",
      "epoch 103, loss 2096.201904296875\n",
      "epoch 104, loss 2087.5380859375\n",
      "epoch 105, loss 2078.91015625\n",
      "epoch 106, loss 2070.318603515625\n",
      "epoch 107, loss 2061.762939453125\n",
      "epoch 108, loss 2053.2431640625\n",
      "epoch 109, loss 2044.7586669921875\n",
      "epoch 110, loss 2036.310546875\n",
      "epoch 111, loss 2027.8970947265625\n",
      "epoch 112, loss 2019.5181884765625\n",
      "epoch 113, loss 2011.1749267578125\n",
      "epoch 114, loss 2002.866943359375\n",
      "epoch 115, loss 1994.59375\n",
      "epoch 116, loss 1986.3548583984375\n",
      "epoch 117, loss 1978.1497802734375\n",
      "epoch 118, loss 1969.9793701171875\n",
      "epoch 119, loss 1961.8426513671875\n",
      "epoch 120, loss 1953.739990234375\n",
      "epoch 121, loss 1945.6712646484375\n",
      "epoch 122, loss 1937.6356201171875\n",
      "epoch 123, loss 1929.6336669921875\n",
      "epoch 124, loss 1921.6654052734375\n",
      "epoch 125, loss 1913.73046875\n",
      "epoch 126, loss 1905.828125\n",
      "epoch 127, loss 1897.95849609375\n",
      "epoch 128, loss 1890.12255859375\n",
      "epoch 129, loss 1882.318115234375\n",
      "epoch 130, loss 1874.5477294921875\n",
      "epoch 131, loss 1866.80908203125\n",
      "epoch 132, loss 1859.1024169921875\n",
      "epoch 133, loss 1851.42724609375\n",
      "epoch 134, loss 1843.78515625\n",
      "epoch 135, loss 1836.173828125\n",
      "epoch 136, loss 1828.5950927734375\n",
      "epoch 137, loss 1821.0477294921875\n",
      "epoch 138, loss 1813.5323486328125\n",
      "epoch 139, loss 1806.0479736328125\n",
      "epoch 140, loss 1798.5941162109375\n",
      "epoch 141, loss 1791.17138671875\n",
      "epoch 142, loss 1783.7791748046875\n",
      "epoch 143, loss 1776.41845703125\n",
      "epoch 144, loss 1769.087890625\n",
      "epoch 145, loss 1761.7877197265625\n",
      "epoch 146, loss 1754.517333984375\n",
      "epoch 147, loss 1747.277587890625\n",
      "epoch 148, loss 1740.068359375\n",
      "epoch 149, loss 1732.8892822265625\n",
      "epoch 150, loss 1725.7403564453125\n",
      "epoch 151, loss 1718.620361328125\n",
      "epoch 152, loss 1711.5301513671875\n",
      "epoch 153, loss 1704.47021484375\n",
      "epoch 154, loss 1697.43896484375\n",
      "epoch 155, loss 1690.4361572265625\n",
      "epoch 156, loss 1683.4635009765625\n",
      "epoch 157, loss 1676.519287109375\n",
      "epoch 158, loss 1669.6041259765625\n",
      "epoch 159, loss 1662.71826171875\n",
      "epoch 160, loss 1655.860595703125\n",
      "epoch 161, loss 1649.031005859375\n",
      "epoch 162, loss 1642.23046875\n",
      "epoch 163, loss 1635.4581298828125\n",
      "epoch 164, loss 1628.713623046875\n",
      "epoch 165, loss 1621.997314453125\n",
      "epoch 166, loss 1615.309326171875\n",
      "epoch 167, loss 1608.6480712890625\n",
      "epoch 168, loss 1602.0152587890625\n",
      "epoch 169, loss 1595.4100341796875\n",
      "epoch 170, loss 1588.8316650390625\n",
      "epoch 171, loss 1582.2806396484375\n",
      "epoch 172, loss 1575.7569580078125\n",
      "epoch 173, loss 1569.2607421875\n",
      "epoch 174, loss 1562.790771484375\n",
      "epoch 175, loss 1556.347900390625\n",
      "epoch 176, loss 1549.93212890625\n",
      "epoch 177, loss 1543.54248046875\n",
      "epoch 178, loss 1537.18017578125\n",
      "epoch 179, loss 1530.84375\n",
      "epoch 180, loss 1524.5338134765625\n",
      "epoch 181, loss 1518.25\n",
      "epoch 182, loss 1511.9918212890625\n",
      "epoch 183, loss 1505.760009765625\n",
      "epoch 184, loss 1499.55419921875\n",
      "epoch 185, loss 1493.37353515625\n",
      "epoch 186, loss 1487.218017578125\n",
      "epoch 187, loss 1481.089111328125\n",
      "epoch 188, loss 1474.9857177734375\n",
      "epoch 189, loss 1468.907470703125\n",
      "epoch 190, loss 1462.853759765625\n",
      "epoch 191, loss 1456.82568359375\n",
      "epoch 192, loss 1450.82275390625\n",
      "epoch 193, loss 1444.8448486328125\n",
      "epoch 194, loss 1438.8917236328125\n",
      "epoch 195, loss 1432.962890625\n",
      "epoch 196, loss 1427.058349609375\n",
      "epoch 197, loss 1421.1788330078125\n",
      "epoch 198, loss 1415.3228759765625\n",
      "epoch 199, loss 1409.491943359375\n",
      "epoch 200, loss 1403.6849365234375\n",
      "epoch 201, loss 1397.902099609375\n",
      "epoch 202, loss 1392.1436767578125\n",
      "epoch 203, loss 1386.40869140625\n",
      "epoch 204, loss 1380.69775390625\n",
      "epoch 205, loss 1375.0096435546875\n",
      "epoch 206, loss 1369.345947265625\n",
      "epoch 207, loss 1363.7052001953125\n",
      "epoch 208, loss 1358.087646484375\n",
      "epoch 209, loss 1352.49365234375\n",
      "epoch 210, loss 1346.9232177734375\n",
      "epoch 211, loss 1341.375244140625\n",
      "epoch 212, loss 1335.85009765625\n",
      "epoch 213, loss 1330.3482666015625\n",
      "epoch 214, loss 1324.868896484375\n",
      "epoch 215, loss 1319.4119873046875\n",
      "epoch 216, loss 1313.9783935546875\n",
      "epoch 217, loss 1308.56689453125\n",
      "epoch 218, loss 1303.177734375\n",
      "epoch 219, loss 1297.810546875\n",
      "epoch 220, loss 1292.4654541015625\n",
      "epoch 221, loss 1287.14306640625\n",
      "epoch 222, loss 1281.8428955078125\n",
      "epoch 223, loss 1276.564453125\n",
      "epoch 224, loss 1271.3074951171875\n",
      "epoch 225, loss 1266.0728759765625\n",
      "epoch 226, loss 1260.859619140625\n",
      "epoch 227, loss 1255.66748046875\n",
      "epoch 228, loss 1250.4974365234375\n",
      "epoch 229, loss 1245.34814453125\n",
      "epoch 230, loss 1240.22021484375\n",
      "epoch 231, loss 1235.113525390625\n",
      "epoch 232, loss 1230.0284423828125\n",
      "epoch 233, loss 1224.9647216796875\n",
      "epoch 234, loss 1219.921630859375\n",
      "epoch 235, loss 1214.8995361328125\n",
      "epoch 236, loss 1209.898681640625\n",
      "epoch 237, loss 1204.91748046875\n",
      "epoch 238, loss 1199.95751953125\n",
      "epoch 239, loss 1195.0177001953125\n",
      "epoch 240, loss 1190.0987548828125\n",
      "epoch 241, loss 1185.2003173828125\n",
      "epoch 242, loss 1180.3214111328125\n",
      "epoch 243, loss 1175.4632568359375\n",
      "epoch 244, loss 1170.625\n",
      "epoch 245, loss 1165.806640625\n",
      "epoch 246, loss 1161.00830078125\n",
      "epoch 247, loss 1156.2294921875\n",
      "epoch 248, loss 1151.4710693359375\n",
      "epoch 249, loss 1146.73193359375\n",
      "epoch 250, loss 1142.011962890625\n",
      "epoch 251, loss 1137.312255859375\n",
      "epoch 252, loss 1132.6314697265625\n",
      "epoch 253, loss 1127.97021484375\n",
      "epoch 254, loss 1123.328369140625\n",
      "epoch 255, loss 1118.705078125\n",
      "epoch 256, loss 1114.1015625\n",
      "epoch 257, loss 1109.516845703125\n",
      "epoch 258, loss 1104.95068359375\n",
      "epoch 259, loss 1100.4036865234375\n",
      "epoch 260, loss 1095.87548828125\n",
      "epoch 261, loss 1091.3660888671875\n",
      "epoch 262, loss 1086.875\n",
      "epoch 263, loss 1082.40283203125\n",
      "epoch 264, loss 1077.94873046875\n",
      "epoch 265, loss 1073.5133056640625\n",
      "epoch 266, loss 1069.096435546875\n",
      "epoch 267, loss 1064.697998046875\n",
      "epoch 268, loss 1060.3172607421875\n",
      "epoch 269, loss 1055.95458984375\n",
      "epoch 270, loss 1051.60986328125\n",
      "epoch 271, loss 1047.28369140625\n",
      "epoch 272, loss 1042.97509765625\n",
      "epoch 273, loss 1038.68408203125\n",
      "epoch 274, loss 1034.41064453125\n",
      "epoch 275, loss 1030.1551513671875\n",
      "epoch 276, loss 1025.9168701171875\n",
      "epoch 277, loss 1021.696533203125\n",
      "epoch 278, loss 1017.4937133789062\n",
      "epoch 279, loss 1013.3082885742188\n",
      "epoch 280, loss 1009.140380859375\n",
      "epoch 281, loss 1004.9892578125\n",
      "epoch 282, loss 1000.855224609375\n",
      "epoch 283, loss 996.7385864257812\n",
      "epoch 284, loss 992.638427734375\n",
      "epoch 285, loss 988.5557861328125\n",
      "epoch 286, loss 984.489990234375\n",
      "epoch 287, loss 980.4405517578125\n",
      "epoch 288, loss 976.408447265625\n",
      "epoch 289, loss 972.392333984375\n",
      "epoch 290, loss 968.392822265625\n",
      "epoch 291, loss 964.409912109375\n",
      "epoch 292, loss 960.4437866210938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 293, loss 956.4932861328125\n",
      "epoch 294, loss 952.5594482421875\n",
      "epoch 295, loss 948.6419067382812\n",
      "epoch 296, loss 944.740478515625\n",
      "epoch 297, loss 940.854736328125\n",
      "epoch 298, loss 936.9852294921875\n",
      "epoch 299, loss 933.1318359375\n",
      "epoch 300, loss 929.2940673828125\n",
      "epoch 301, loss 925.472412109375\n",
      "epoch 302, loss 921.666259765625\n",
      "epoch 303, loss 917.8760986328125\n",
      "epoch 304, loss 914.1018676757812\n",
      "epoch 305, loss 910.3428955078125\n",
      "epoch 306, loss 906.5989379882812\n",
      "epoch 307, loss 902.8705444335938\n",
      "epoch 308, loss 899.1580810546875\n",
      "epoch 309, loss 895.460693359375\n",
      "epoch 310, loss 891.7786254882812\n",
      "epoch 311, loss 888.1118774414062\n",
      "epoch 312, loss 884.4600830078125\n",
      "epoch 313, loss 880.823486328125\n",
      "epoch 314, loss 877.201416015625\n",
      "epoch 315, loss 873.5945434570312\n",
      "epoch 316, loss 870.0022583007812\n",
      "epoch 317, loss 866.4249267578125\n",
      "epoch 318, loss 862.8626098632812\n",
      "epoch 319, loss 859.3147583007812\n",
      "epoch 320, loss 855.781494140625\n",
      "epoch 321, loss 852.2630615234375\n",
      "epoch 322, loss 848.7593994140625\n",
      "epoch 323, loss 845.2695922851562\n",
      "epoch 324, loss 841.794677734375\n",
      "epoch 325, loss 838.3334350585938\n",
      "epoch 326, loss 834.8866577148438\n",
      "epoch 327, loss 831.4542846679688\n",
      "epoch 328, loss 828.0362548828125\n",
      "epoch 329, loss 824.6322631835938\n",
      "epoch 330, loss 821.2422485351562\n",
      "epoch 331, loss 817.8660888671875\n",
      "epoch 332, loss 814.503662109375\n",
      "epoch 333, loss 811.1555786132812\n",
      "epoch 334, loss 807.8211059570312\n",
      "epoch 335, loss 804.5006103515625\n",
      "epoch 336, loss 801.193359375\n",
      "epoch 337, loss 797.9002075195312\n",
      "epoch 338, loss 794.6204833984375\n",
      "epoch 339, loss 791.3538818359375\n",
      "epoch 340, loss 788.1008911132812\n",
      "epoch 341, loss 784.8617553710938\n",
      "epoch 342, loss 781.6354370117188\n",
      "epoch 343, loss 778.4228515625\n",
      "epoch 344, loss 775.2233276367188\n",
      "epoch 345, loss 772.0368041992188\n",
      "epoch 346, loss 768.863525390625\n",
      "epoch 347, loss 765.7035522460938\n",
      "epoch 348, loss 762.5562744140625\n",
      "epoch 349, loss 759.4215698242188\n",
      "epoch 350, loss 756.3002319335938\n",
      "epoch 351, loss 753.1918334960938\n",
      "epoch 352, loss 750.0958251953125\n",
      "epoch 353, loss 747.0130004882812\n",
      "epoch 354, loss 743.9429321289062\n",
      "epoch 355, loss 740.885498046875\n",
      "epoch 356, loss 737.840576171875\n",
      "epoch 357, loss 734.8079223632812\n",
      "epoch 358, loss 731.7882690429688\n",
      "epoch 359, loss 728.7809448242188\n",
      "epoch 360, loss 725.78564453125\n",
      "epoch 361, loss 722.8028564453125\n",
      "epoch 362, loss 719.8322143554688\n",
      "epoch 363, loss 716.8740844726562\n",
      "epoch 364, loss 713.927978515625\n",
      "epoch 365, loss 710.993896484375\n",
      "epoch 366, loss 708.072265625\n",
      "epoch 367, loss 705.1625366210938\n",
      "epoch 368, loss 702.2648315429688\n",
      "epoch 369, loss 699.37890625\n",
      "epoch 370, loss 696.505126953125\n",
      "epoch 371, loss 693.642822265625\n",
      "epoch 372, loss 690.7924194335938\n",
      "epoch 373, loss 687.9536743164062\n",
      "epoch 374, loss 685.126708984375\n",
      "epoch 375, loss 682.3114624023438\n",
      "epoch 376, loss 679.508056640625\n",
      "epoch 377, loss 676.7161254882812\n",
      "epoch 378, loss 673.9356079101562\n",
      "epoch 379, loss 671.166259765625\n",
      "epoch 380, loss 668.4083251953125\n",
      "epoch 381, loss 665.6616821289062\n",
      "epoch 382, loss 662.9263305664062\n",
      "epoch 383, loss 660.2024536132812\n",
      "epoch 384, loss 657.489990234375\n",
      "epoch 385, loss 654.7883911132812\n",
      "epoch 386, loss 652.0980834960938\n",
      "epoch 387, loss 649.4188232421875\n",
      "epoch 388, loss 646.75048828125\n",
      "epoch 389, loss 644.0932006835938\n",
      "epoch 390, loss 641.4471435546875\n",
      "epoch 391, loss 638.811767578125\n",
      "epoch 392, loss 636.1873779296875\n",
      "epoch 393, loss 633.5739135742188\n",
      "epoch 394, loss 630.9708251953125\n",
      "epoch 395, loss 628.3786010742188\n",
      "epoch 396, loss 625.7971801757812\n",
      "epoch 397, loss 623.226318359375\n",
      "epoch 398, loss 620.6658935546875\n",
      "epoch 399, loss 618.115966796875\n",
      "epoch 400, loss 615.576416015625\n",
      "epoch 401, loss 613.0474243164062\n",
      "epoch 402, loss 610.529052734375\n",
      "epoch 403, loss 608.0210571289062\n",
      "epoch 404, loss 605.5234375\n",
      "epoch 405, loss 603.035888671875\n",
      "epoch 406, loss 600.5586547851562\n",
      "epoch 407, loss 598.0914916992188\n",
      "epoch 408, loss 595.6347045898438\n",
      "epoch 409, loss 593.1880493164062\n",
      "epoch 410, loss 590.7513427734375\n",
      "epoch 411, loss 588.3246459960938\n",
      "epoch 412, loss 585.908203125\n",
      "epoch 413, loss 583.5015258789062\n",
      "epoch 414, loss 581.1046142578125\n",
      "epoch 415, loss 578.7174682617188\n",
      "epoch 416, loss 576.3402099609375\n",
      "epoch 417, loss 573.972900390625\n",
      "epoch 418, loss 571.6150512695312\n",
      "epoch 419, loss 569.2669067382812\n",
      "epoch 420, loss 566.9287109375\n",
      "epoch 421, loss 564.6000366210938\n",
      "epoch 422, loss 562.2808227539062\n",
      "epoch 423, loss 559.97119140625\n",
      "epoch 424, loss 557.6710205078125\n",
      "epoch 425, loss 555.3804931640625\n",
      "epoch 426, loss 553.099365234375\n",
      "epoch 427, loss 550.8274536132812\n",
      "epoch 428, loss 548.5650024414062\n",
      "epoch 429, loss 546.3119506835938\n",
      "epoch 430, loss 544.068115234375\n",
      "epoch 431, loss 541.8336181640625\n",
      "epoch 432, loss 539.6082763671875\n",
      "epoch 433, loss 537.3920288085938\n",
      "epoch 434, loss 535.184814453125\n",
      "epoch 435, loss 532.9869384765625\n",
      "epoch 436, loss 530.798095703125\n",
      "epoch 437, loss 528.617919921875\n",
      "epoch 438, loss 526.447021484375\n",
      "epoch 439, loss 524.2849731445312\n",
      "epoch 440, loss 522.1318969726562\n",
      "epoch 441, loss 519.987548828125\n",
      "epoch 442, loss 517.8519897460938\n",
      "epoch 443, loss 515.7252807617188\n",
      "epoch 444, loss 513.607177734375\n",
      "epoch 445, loss 511.4979248046875\n",
      "epoch 446, loss 509.3973388671875\n",
      "epoch 447, loss 507.3053283691406\n",
      "epoch 448, loss 505.2220153808594\n",
      "epoch 449, loss 503.1471862792969\n",
      "epoch 450, loss 501.0809020996094\n",
      "epoch 451, loss 499.0232238769531\n",
      "epoch 452, loss 496.9738464355469\n",
      "epoch 453, loss 494.9328308105469\n",
      "epoch 454, loss 492.9001770019531\n",
      "epoch 455, loss 490.8760986328125\n",
      "epoch 456, loss 488.86041259765625\n",
      "epoch 457, loss 486.8528747558594\n",
      "epoch 458, loss 484.8537292480469\n",
      "epoch 459, loss 482.8626403808594\n",
      "epoch 460, loss 480.87994384765625\n",
      "epoch 461, loss 478.90533447265625\n",
      "epoch 462, loss 476.9386291503906\n",
      "epoch 463, loss 474.9801940917969\n",
      "epoch 464, loss 473.0300598144531\n",
      "epoch 465, loss 471.0877685546875\n",
      "epoch 466, loss 469.1534729003906\n",
      "epoch 467, loss 467.2272033691406\n",
      "epoch 468, loss 465.3087158203125\n",
      "epoch 469, loss 463.39801025390625\n",
      "epoch 470, loss 461.4952697753906\n",
      "epoch 471, loss 459.6004333496094\n",
      "epoch 472, loss 457.7131042480469\n",
      "epoch 473, loss 455.8337097167969\n",
      "epoch 474, loss 453.9620666503906\n",
      "epoch 475, loss 452.0981750488281\n",
      "epoch 476, loss 450.2418212890625\n",
      "epoch 477, loss 448.3930358886719\n",
      "epoch 478, loss 446.5521545410156\n",
      "epoch 479, loss 444.7187194824219\n",
      "epoch 480, loss 442.8928527832031\n",
      "epoch 481, loss 441.0745544433594\n",
      "epoch 482, loss 439.26361083984375\n",
      "epoch 483, loss 437.4601745605469\n",
      "epoch 484, loss 435.66400146484375\n",
      "epoch 485, loss 433.8753967285156\n",
      "epoch 486, loss 432.0939636230469\n",
      "epoch 487, loss 430.3199768066406\n",
      "epoch 488, loss 428.5533447265625\n",
      "epoch 489, loss 426.7940673828125\n",
      "epoch 490, loss 425.04180908203125\n",
      "epoch 491, loss 423.296875\n",
      "epoch 492, loss 421.5590515136719\n",
      "epoch 493, loss 419.8283996582031\n",
      "epoch 494, loss 418.10491943359375\n",
      "epoch 495, loss 416.3883972167969\n",
      "epoch 496, loss 414.6789855957031\n",
      "epoch 497, loss 412.9765319824219\n",
      "epoch 498, loss 411.2811279296875\n",
      "epoch 499, loss 409.59283447265625\n",
      "epoch 500, loss 407.9112548828125\n",
      "epoch 501, loss 406.2366943359375\n",
      "epoch 502, loss 404.56915283203125\n",
      "epoch 503, loss 402.9083251953125\n",
      "epoch 504, loss 401.2544860839844\n",
      "epoch 505, loss 399.6072692871094\n",
      "epoch 506, loss 397.9667663574219\n",
      "epoch 507, loss 396.3330383300781\n",
      "epoch 508, loss 394.70611572265625\n",
      "epoch 509, loss 393.08587646484375\n",
      "epoch 510, loss 391.47222900390625\n",
      "epoch 511, loss 389.8653564453125\n",
      "epoch 512, loss 388.2650146484375\n",
      "epoch 513, loss 386.6711730957031\n",
      "epoch 514, loss 385.0838623046875\n",
      "epoch 515, loss 383.5031433105469\n",
      "epoch 516, loss 381.9288635253906\n",
      "epoch 517, loss 380.361083984375\n",
      "epoch 518, loss 378.79986572265625\n",
      "epoch 519, loss 377.2452697753906\n",
      "epoch 520, loss 375.69696044921875\n",
      "epoch 521, loss 374.1549072265625\n",
      "epoch 522, loss 372.61907958984375\n",
      "epoch 523, loss 371.0896301269531\n",
      "epoch 524, loss 369.56646728515625\n",
      "epoch 525, loss 368.0496826171875\n",
      "epoch 526, loss 366.5390625\n",
      "epoch 527, loss 365.03466796875\n",
      "epoch 528, loss 363.5363464355469\n",
      "epoch 529, loss 362.0443420410156\n",
      "epoch 530, loss 360.5582580566406\n",
      "epoch 531, loss 359.0784606933594\n",
      "epoch 532, loss 357.60479736328125\n",
      "epoch 533, loss 356.13714599609375\n",
      "epoch 534, loss 354.67547607421875\n",
      "epoch 535, loss 353.2197265625\n",
      "epoch 536, loss 351.7701416015625\n",
      "epoch 537, loss 350.3262939453125\n",
      "epoch 538, loss 348.8885192871094\n",
      "epoch 539, loss 347.4566650390625\n",
      "epoch 540, loss 346.0307312011719\n",
      "epoch 541, loss 344.6106262207031\n",
      "epoch 542, loss 343.19622802734375\n",
      "epoch 543, loss 341.7878112792969\n",
      "epoch 544, loss 340.3851623535156\n",
      "epoch 545, loss 338.98828125\n",
      "epoch 546, loss 337.5970458984375\n",
      "epoch 547, loss 336.2115783691406\n",
      "epoch 548, loss 334.83184814453125\n",
      "epoch 549, loss 333.4576721191406\n",
      "epoch 550, loss 332.0892333984375\n",
      "epoch 551, loss 330.7262268066406\n",
      "epoch 552, loss 329.3689880371094\n",
      "epoch 553, loss 328.0173645019531\n",
      "epoch 554, loss 326.6712646484375\n",
      "epoch 555, loss 325.3306579589844\n",
      "epoch 556, loss 323.99566650390625\n",
      "epoch 557, loss 322.6662292480469\n",
      "epoch 558, loss 321.3421936035156\n",
      "epoch 559, loss 320.0235595703125\n",
      "epoch 560, loss 318.71026611328125\n",
      "epoch 561, loss 317.4024658203125\n",
      "epoch 562, loss 316.0999450683594\n",
      "epoch 563, loss 314.8028564453125\n",
      "epoch 564, loss 313.51116943359375\n",
      "epoch 565, loss 312.2248840332031\n",
      "epoch 566, loss 310.94378662109375\n",
      "epoch 567, loss 309.6676940917969\n",
      "epoch 568, loss 308.3970947265625\n",
      "epoch 569, loss 307.131591796875\n",
      "epoch 570, loss 305.8713684082031\n",
      "epoch 571, loss 304.6163635253906\n",
      "epoch 572, loss 303.366455078125\n",
      "epoch 573, loss 302.1217346191406\n",
      "epoch 574, loss 300.8821105957031\n",
      "epoch 575, loss 299.6475524902344\n",
      "epoch 576, loss 298.4180908203125\n",
      "epoch 577, loss 297.1935119628906\n",
      "epoch 578, loss 295.9742431640625\n",
      "epoch 579, loss 294.7598876953125\n",
      "epoch 580, loss 293.5505676269531\n",
      "epoch 581, loss 292.3460693359375\n",
      "epoch 582, loss 291.1466979980469\n",
      "epoch 583, loss 289.9521484375\n",
      "epoch 584, loss 288.76239013671875\n",
      "epoch 585, loss 287.57763671875\n",
      "epoch 586, loss 286.3978271484375\n",
      "epoch 587, loss 285.2228088378906\n",
      "epoch 588, loss 284.0527038574219\n",
      "epoch 589, loss 282.8873291015625\n",
      "epoch 590, loss 281.7266845703125\n",
      "epoch 591, loss 280.57080078125\n",
      "epoch 592, loss 279.4196472167969\n",
      "epoch 593, loss 278.2734375\n",
      "epoch 594, loss 277.13189697265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 595, loss 275.9949645996094\n",
      "epoch 596, loss 274.8626403808594\n",
      "epoch 597, loss 273.7350158691406\n",
      "epoch 598, loss 272.6119384765625\n",
      "epoch 599, loss 271.49359130859375\n",
      "epoch 600, loss 270.3798828125\n",
      "epoch 601, loss 269.2706604003906\n",
      "epoch 602, loss 268.16595458984375\n",
      "epoch 603, loss 267.06591796875\n",
      "epoch 604, loss 265.9703674316406\n",
      "epoch 605, loss 264.8792419433594\n",
      "epoch 606, loss 263.7926025390625\n",
      "epoch 607, loss 262.71044921875\n",
      "epoch 608, loss 261.6329345703125\n",
      "epoch 609, loss 260.55975341796875\n",
      "epoch 610, loss 259.4908447265625\n",
      "epoch 611, loss 258.42633056640625\n",
      "epoch 612, loss 257.36627197265625\n",
      "epoch 613, loss 256.310546875\n",
      "epoch 614, loss 255.2592010498047\n",
      "epoch 615, loss 254.21212768554688\n",
      "epoch 616, loss 253.1692657470703\n",
      "epoch 617, loss 252.13082885742188\n",
      "epoch 618, loss 251.0965118408203\n",
      "epoch 619, loss 250.06661987304688\n",
      "epoch 620, loss 249.04090881347656\n",
      "epoch 621, loss 248.0193634033203\n",
      "epoch 622, loss 247.001953125\n",
      "epoch 623, loss 245.9886932373047\n",
      "epoch 624, loss 244.9796600341797\n",
      "epoch 625, loss 243.9747772216797\n",
      "epoch 626, loss 242.97418212890625\n",
      "epoch 627, loss 241.97760009765625\n",
      "epoch 628, loss 240.98497009277344\n",
      "epoch 629, loss 239.99655151367188\n",
      "epoch 630, loss 239.01210021972656\n",
      "epoch 631, loss 238.03184509277344\n",
      "epoch 632, loss 237.05560302734375\n",
      "epoch 633, loss 236.0833282470703\n",
      "epoch 634, loss 235.11517333984375\n",
      "epoch 635, loss 234.15093994140625\n",
      "epoch 636, loss 233.19070434570312\n",
      "epoch 637, loss 232.234375\n",
      "epoch 638, loss 231.28184509277344\n",
      "epoch 639, loss 230.33331298828125\n",
      "epoch 640, loss 229.388671875\n",
      "epoch 641, loss 228.44793701171875\n",
      "epoch 642, loss 227.5111541748047\n",
      "epoch 643, loss 226.5780487060547\n",
      "epoch 644, loss 225.6487579345703\n",
      "epoch 645, loss 224.72340393066406\n",
      "epoch 646, loss 223.80181884765625\n",
      "epoch 647, loss 222.88404846191406\n",
      "epoch 648, loss 221.96990966796875\n",
      "epoch 649, loss 221.05953979492188\n",
      "epoch 650, loss 220.1529998779297\n",
      "epoch 651, loss 219.2501678466797\n",
      "epoch 652, loss 218.35105895996094\n",
      "epoch 653, loss 217.4557342529297\n",
      "epoch 654, loss 216.56399536132812\n",
      "epoch 655, loss 215.6759033203125\n",
      "epoch 656, loss 214.79147338867188\n",
      "epoch 657, loss 213.91067504882812\n",
      "epoch 658, loss 213.03334045410156\n",
      "epoch 659, loss 212.15972900390625\n",
      "epoch 660, loss 211.28968811035156\n",
      "epoch 661, loss 210.42332458496094\n",
      "epoch 662, loss 209.56048583984375\n",
      "epoch 663, loss 208.7012176513672\n",
      "epoch 664, loss 207.845458984375\n",
      "epoch 665, loss 206.99313354492188\n",
      "epoch 666, loss 206.14453125\n",
      "epoch 667, loss 205.29925537109375\n",
      "epoch 668, loss 204.45748901367188\n",
      "epoch 669, loss 203.61912536621094\n",
      "epoch 670, loss 202.78428649902344\n",
      "epoch 671, loss 201.9528045654297\n",
      "epoch 672, loss 201.12481689453125\n",
      "epoch 673, loss 200.30015563964844\n",
      "epoch 674, loss 199.47886657714844\n",
      "epoch 675, loss 198.6609649658203\n",
      "epoch 676, loss 197.84637451171875\n",
      "epoch 677, loss 197.0352020263672\n",
      "epoch 678, loss 196.22727966308594\n",
      "epoch 679, loss 195.42279052734375\n",
      "epoch 680, loss 194.62164306640625\n",
      "epoch 681, loss 193.82369995117188\n",
      "epoch 682, loss 193.02906799316406\n",
      "epoch 683, loss 192.23765563964844\n",
      "epoch 684, loss 191.44952392578125\n",
      "epoch 685, loss 190.66456604003906\n",
      "epoch 686, loss 189.88290405273438\n",
      "epoch 687, loss 189.10450744628906\n",
      "epoch 688, loss 188.32933044433594\n",
      "epoch 689, loss 187.55723571777344\n",
      "epoch 690, loss 186.78826904296875\n",
      "epoch 691, loss 186.02252197265625\n",
      "epoch 692, loss 185.25994873046875\n",
      "epoch 693, loss 184.50042724609375\n",
      "epoch 694, loss 183.74403381347656\n",
      "epoch 695, loss 182.99078369140625\n",
      "epoch 696, loss 182.24058532714844\n",
      "epoch 697, loss 181.49349975585938\n",
      "epoch 698, loss 180.74964904785156\n",
      "epoch 699, loss 180.0087432861328\n",
      "epoch 700, loss 179.27090454101562\n",
      "epoch 701, loss 178.5360565185547\n",
      "epoch 702, loss 177.8042755126953\n",
      "epoch 703, loss 177.07542419433594\n",
      "epoch 704, loss 176.3495635986328\n",
      "epoch 705, loss 175.626708984375\n",
      "epoch 706, loss 174.90684509277344\n",
      "epoch 707, loss 174.19000244140625\n",
      "epoch 708, loss 173.4761199951172\n",
      "epoch 709, loss 172.76507568359375\n",
      "epoch 710, loss 172.05699157714844\n",
      "epoch 711, loss 171.35177612304688\n",
      "epoch 712, loss 170.64947509765625\n",
      "epoch 713, loss 169.95001220703125\n",
      "epoch 714, loss 169.25341796875\n",
      "epoch 715, loss 168.5597381591797\n",
      "epoch 716, loss 167.868896484375\n",
      "epoch 717, loss 167.1808624267578\n",
      "epoch 718, loss 166.4956817626953\n",
      "epoch 719, loss 165.81333923339844\n",
      "epoch 720, loss 165.13375854492188\n",
      "epoch 721, loss 164.45697021484375\n",
      "epoch 722, loss 163.78289794921875\n",
      "epoch 723, loss 163.11167907714844\n",
      "epoch 724, loss 162.44322204589844\n",
      "epoch 725, loss 161.7775115966797\n",
      "epoch 726, loss 161.11453247070312\n",
      "epoch 727, loss 160.454345703125\n",
      "epoch 728, loss 159.79681396484375\n",
      "epoch 729, loss 159.1419677734375\n",
      "epoch 730, loss 158.48971557617188\n",
      "epoch 731, loss 157.84027099609375\n",
      "epoch 732, loss 157.19342041015625\n",
      "epoch 733, loss 156.5492401123047\n",
      "epoch 734, loss 155.9077911376953\n",
      "epoch 735, loss 155.26889038085938\n",
      "epoch 736, loss 154.63262939453125\n",
      "epoch 737, loss 153.9989776611328\n",
      "epoch 738, loss 153.367919921875\n",
      "epoch 739, loss 152.73948669433594\n",
      "epoch 740, loss 152.1136016845703\n",
      "epoch 741, loss 151.49032592773438\n",
      "epoch 742, loss 150.86956787109375\n",
      "epoch 743, loss 150.2514190673828\n",
      "epoch 744, loss 149.63580322265625\n",
      "epoch 745, loss 149.02273559570312\n",
      "epoch 746, loss 148.41217041015625\n",
      "epoch 747, loss 147.80409240722656\n",
      "epoch 748, loss 147.19850158691406\n",
      "epoch 749, loss 146.59535217285156\n",
      "epoch 750, loss 145.99468994140625\n",
      "epoch 751, loss 145.39659118652344\n",
      "epoch 752, loss 144.80087280273438\n",
      "epoch 753, loss 144.2076416015625\n",
      "epoch 754, loss 143.61683654785156\n",
      "epoch 755, loss 143.02845764160156\n",
      "epoch 756, loss 142.44252014160156\n",
      "epoch 757, loss 141.8589630126953\n",
      "epoch 758, loss 141.27783203125\n",
      "epoch 759, loss 140.69898986816406\n",
      "epoch 760, loss 140.1226043701172\n",
      "epoch 761, loss 139.54864501953125\n",
      "epoch 762, loss 138.97694396972656\n",
      "epoch 763, loss 138.4076385498047\n",
      "epoch 764, loss 137.8406219482422\n",
      "epoch 765, loss 137.27593994140625\n",
      "epoch 766, loss 136.71348571777344\n",
      "epoch 767, loss 136.15341186523438\n",
      "epoch 768, loss 135.59568786621094\n",
      "epoch 769, loss 135.04025268554688\n",
      "epoch 770, loss 134.48712158203125\n",
      "epoch 771, loss 133.9362030029297\n",
      "epoch 772, loss 133.3874969482422\n",
      "epoch 773, loss 132.8411102294922\n",
      "epoch 774, loss 132.2969970703125\n",
      "epoch 775, loss 131.75511169433594\n",
      "epoch 776, loss 131.21543884277344\n",
      "epoch 777, loss 130.67800903320312\n",
      "epoch 778, loss 130.1427459716797\n",
      "epoch 779, loss 129.6096954345703\n",
      "epoch 780, loss 129.078857421875\n",
      "epoch 781, loss 128.55020141601562\n",
      "epoch 782, loss 128.023681640625\n",
      "epoch 783, loss 127.49940490722656\n",
      "epoch 784, loss 126.97715759277344\n",
      "epoch 785, loss 126.45713806152344\n",
      "epoch 786, loss 125.93922424316406\n",
      "epoch 787, loss 125.42337799072266\n",
      "epoch 788, loss 124.90975952148438\n",
      "epoch 789, loss 124.398193359375\n",
      "epoch 790, loss 123.88877868652344\n",
      "epoch 791, loss 123.38140106201172\n",
      "epoch 792, loss 122.87616729736328\n",
      "epoch 793, loss 122.37299346923828\n",
      "epoch 794, loss 121.871826171875\n",
      "epoch 795, loss 121.37277221679688\n",
      "epoch 796, loss 120.87577056884766\n",
      "epoch 797, loss 120.38080596923828\n",
      "epoch 798, loss 119.88790893554688\n",
      "epoch 799, loss 119.39695739746094\n",
      "epoch 800, loss 118.90806579589844\n",
      "epoch 801, loss 118.421142578125\n",
      "epoch 802, loss 117.93614196777344\n",
      "epoch 803, loss 117.4532470703125\n",
      "epoch 804, loss 116.97230529785156\n",
      "epoch 805, loss 116.49332427978516\n",
      "epoch 806, loss 116.01637268066406\n",
      "epoch 807, loss 115.54136657714844\n",
      "epoch 808, loss 115.06830596923828\n",
      "epoch 809, loss 114.59716796875\n",
      "epoch 810, loss 114.12802124023438\n",
      "epoch 811, loss 113.66075134277344\n",
      "epoch 812, loss 113.19544219970703\n",
      "epoch 813, loss 112.73202514648438\n",
      "epoch 814, loss 112.2705078125\n",
      "epoch 815, loss 111.81086730957031\n",
      "epoch 816, loss 111.3531494140625\n",
      "epoch 817, loss 110.89730834960938\n",
      "epoch 818, loss 110.44332122802734\n",
      "epoch 819, loss 109.99118041992188\n",
      "epoch 820, loss 109.54088592529297\n",
      "epoch 821, loss 109.09242248535156\n",
      "epoch 822, loss 108.64584350585938\n",
      "epoch 823, loss 108.20108795166016\n",
      "epoch 824, loss 107.75818634033203\n",
      "epoch 825, loss 107.3171157836914\n",
      "epoch 826, loss 106.87779235839844\n",
      "epoch 827, loss 106.44033813476562\n",
      "epoch 828, loss 106.00462341308594\n",
      "epoch 829, loss 105.57076263427734\n",
      "epoch 830, loss 105.1385726928711\n",
      "epoch 831, loss 104.70822143554688\n",
      "epoch 832, loss 104.2796401977539\n",
      "epoch 833, loss 103.85286712646484\n",
      "epoch 834, loss 103.42781066894531\n",
      "epoch 835, loss 103.00457763671875\n",
      "epoch 836, loss 102.58295440673828\n",
      "epoch 837, loss 102.16316223144531\n",
      "epoch 838, loss 101.74502563476562\n",
      "epoch 839, loss 101.32857513427734\n",
      "epoch 840, loss 100.91387939453125\n",
      "epoch 841, loss 100.50088500976562\n",
      "epoch 842, loss 100.08966827392578\n",
      "epoch 843, loss 99.68013763427734\n",
      "epoch 844, loss 99.27223205566406\n",
      "epoch 845, loss 98.86599731445312\n",
      "epoch 846, loss 98.46147918701172\n",
      "epoch 847, loss 98.05860900878906\n",
      "epoch 848, loss 97.65737915039062\n",
      "epoch 849, loss 97.25776672363281\n",
      "epoch 850, loss 96.85984802246094\n",
      "epoch 851, loss 96.46359252929688\n",
      "epoch 852, loss 96.06887817382812\n",
      "epoch 853, loss 95.67579650878906\n",
      "epoch 854, loss 95.28431701660156\n",
      "epoch 855, loss 94.89439392089844\n",
      "epoch 856, loss 94.50616455078125\n",
      "epoch 857, loss 94.11947631835938\n",
      "epoch 858, loss 93.73442840576172\n",
      "epoch 859, loss 93.3509521484375\n",
      "epoch 860, loss 92.96900939941406\n",
      "epoch 861, loss 92.588623046875\n",
      "epoch 862, loss 92.20986938476562\n",
      "epoch 863, loss 91.83262634277344\n",
      "epoch 864, loss 91.45694732666016\n",
      "epoch 865, loss 91.08277893066406\n",
      "epoch 866, loss 90.71022033691406\n",
      "epoch 867, loss 90.33914947509766\n",
      "epoch 868, loss 89.96965789794922\n",
      "epoch 869, loss 89.60165405273438\n",
      "epoch 870, loss 89.23513793945312\n",
      "epoch 871, loss 88.87007904052734\n",
      "epoch 872, loss 88.50652313232422\n",
      "epoch 873, loss 88.14451599121094\n",
      "epoch 874, loss 87.78397369384766\n",
      "epoch 875, loss 87.42491149902344\n",
      "epoch 876, loss 87.06735229492188\n",
      "epoch 877, loss 86.71122741699219\n",
      "epoch 878, loss 86.35662078857422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 879, loss 86.00341796875\n",
      "epoch 880, loss 85.65167999267578\n",
      "epoch 881, loss 85.30139923095703\n",
      "epoch 882, loss 84.95254516601562\n",
      "epoch 883, loss 84.60513305664062\n",
      "epoch 884, loss 84.25911712646484\n",
      "epoch 885, loss 83.91453552246094\n",
      "epoch 886, loss 83.57135772705078\n",
      "epoch 887, loss 83.22962188720703\n",
      "epoch 888, loss 82.88932037353516\n",
      "epoch 889, loss 82.55036163330078\n",
      "epoch 890, loss 82.21280670166016\n",
      "epoch 891, loss 81.87661743164062\n",
      "epoch 892, loss 81.54178619384766\n",
      "epoch 893, loss 81.20832061767578\n",
      "epoch 894, loss 80.87628936767578\n",
      "epoch 895, loss 80.54560852050781\n",
      "epoch 896, loss 80.21622467041016\n",
      "epoch 897, loss 79.88829040527344\n",
      "epoch 898, loss 79.56171417236328\n",
      "epoch 899, loss 79.23641967773438\n",
      "epoch 900, loss 78.91249084472656\n",
      "epoch 901, loss 78.58990478515625\n",
      "epoch 902, loss 78.26866912841797\n",
      "epoch 903, loss 77.94873809814453\n",
      "epoch 904, loss 77.63003540039062\n",
      "epoch 905, loss 77.31265258789062\n",
      "epoch 906, loss 76.99659729003906\n",
      "epoch 907, loss 76.68192291259766\n",
      "epoch 908, loss 76.36847686767578\n",
      "epoch 909, loss 76.05631256103516\n",
      "epoch 910, loss 75.74539184570312\n",
      "epoch 911, loss 75.43572998046875\n",
      "epoch 912, loss 75.12738800048828\n",
      "epoch 913, loss 74.82032775878906\n",
      "epoch 914, loss 74.5145263671875\n",
      "epoch 915, loss 74.2099380493164\n",
      "epoch 916, loss 73.90670776367188\n",
      "epoch 917, loss 73.60464477539062\n",
      "epoch 918, loss 73.30387878417969\n",
      "epoch 919, loss 73.00431060791016\n",
      "epoch 920, loss 72.7059326171875\n",
      "epoch 921, loss 72.40885162353516\n",
      "epoch 922, loss 72.11299896240234\n",
      "epoch 923, loss 71.81829071044922\n",
      "epoch 924, loss 71.52484130859375\n",
      "epoch 925, loss 71.23262786865234\n",
      "epoch 926, loss 70.94154357910156\n",
      "epoch 927, loss 70.65164184570312\n",
      "epoch 928, loss 70.36293029785156\n",
      "epoch 929, loss 70.07538604736328\n",
      "epoch 930, loss 69.7890625\n",
      "epoch 931, loss 69.50393676757812\n",
      "epoch 932, loss 69.219970703125\n",
      "epoch 933, loss 68.93714904785156\n",
      "epoch 934, loss 68.65544891357422\n",
      "epoch 935, loss 68.37500762939453\n",
      "epoch 936, loss 68.09566497802734\n",
      "epoch 937, loss 67.81747436523438\n",
      "epoch 938, loss 67.5404281616211\n",
      "epoch 939, loss 67.26447296142578\n",
      "epoch 940, loss 66.98973083496094\n",
      "epoch 941, loss 66.71609497070312\n",
      "epoch 942, loss 66.44359588623047\n",
      "epoch 943, loss 66.17225646972656\n",
      "epoch 944, loss 65.90199279785156\n",
      "epoch 945, loss 65.63284301757812\n",
      "epoch 946, loss 65.36478424072266\n",
      "epoch 947, loss 65.09779357910156\n",
      "epoch 948, loss 64.8319091796875\n",
      "epoch 949, loss 64.56713104248047\n",
      "epoch 950, loss 64.30343627929688\n",
      "epoch 951, loss 64.04080200195312\n",
      "epoch 952, loss 63.77923583984375\n",
      "epoch 953, loss 63.518798828125\n",
      "epoch 954, loss 63.25941467285156\n",
      "epoch 955, loss 63.00105667114258\n",
      "epoch 956, loss 62.74380111694336\n",
      "epoch 957, loss 62.48759078979492\n",
      "epoch 958, loss 62.23243713378906\n",
      "epoch 959, loss 61.97833251953125\n",
      "epoch 960, loss 61.72531509399414\n",
      "epoch 961, loss 61.47328567504883\n",
      "epoch 962, loss 61.22230529785156\n",
      "epoch 963, loss 60.97234344482422\n",
      "epoch 964, loss 60.72348403930664\n",
      "epoch 965, loss 60.47561264038086\n",
      "epoch 966, loss 60.22870635986328\n",
      "epoch 967, loss 59.98278045654297\n",
      "epoch 968, loss 59.73788833618164\n",
      "epoch 969, loss 59.49399185180664\n",
      "epoch 970, loss 59.25116729736328\n",
      "epoch 971, loss 59.00931930541992\n",
      "epoch 972, loss 58.76845169067383\n",
      "epoch 973, loss 58.52854919433594\n",
      "epoch 974, loss 58.28969955444336\n",
      "epoch 975, loss 58.05181121826172\n",
      "epoch 976, loss 57.81486511230469\n",
      "epoch 977, loss 57.57891082763672\n",
      "epoch 978, loss 57.34392547607422\n",
      "epoch 979, loss 57.10990905761719\n",
      "epoch 980, loss 56.87685775756836\n",
      "epoch 981, loss 56.64470672607422\n",
      "epoch 982, loss 56.41352844238281\n",
      "epoch 983, loss 56.183345794677734\n",
      "epoch 984, loss 55.95408630371094\n",
      "epoch 985, loss 55.72581100463867\n",
      "epoch 986, loss 55.49845504760742\n",
      "epoch 987, loss 55.272003173828125\n",
      "epoch 988, loss 55.04652404785156\n",
      "epoch 989, loss 54.82196044921875\n",
      "epoch 990, loss 54.59830856323242\n",
      "epoch 991, loss 54.37553405761719\n",
      "epoch 992, loss 54.15367126464844\n",
      "epoch 993, loss 53.932777404785156\n",
      "epoch 994, loss 53.71275329589844\n",
      "epoch 995, loss 53.493648529052734\n",
      "epoch 996, loss 53.27545928955078\n",
      "epoch 997, loss 53.05811309814453\n",
      "epoch 998, loss 52.84165954589844\n",
      "epoch 999, loss 52.626102447509766\n",
      "epoch 1000, loss 52.41143035888672\n",
      "epoch 1001, loss 52.19764709472656\n",
      "epoch 1002, loss 51.98477554321289\n",
      "epoch 1003, loss 51.772743225097656\n",
      "epoch 1004, loss 51.56157684326172\n",
      "epoch 1005, loss 51.35129165649414\n",
      "epoch 1006, loss 51.141845703125\n",
      "epoch 1007, loss 50.93333435058594\n",
      "epoch 1008, loss 50.72568130493164\n",
      "epoch 1009, loss 50.51884078979492\n",
      "epoch 1010, loss 50.312843322753906\n",
      "epoch 1011, loss 50.107669830322266\n",
      "epoch 1012, loss 49.90335464477539\n",
      "epoch 1013, loss 49.69987106323242\n",
      "epoch 1014, loss 49.49721145629883\n",
      "epoch 1015, loss 49.29539489746094\n",
      "epoch 1016, loss 49.09444046020508\n",
      "epoch 1017, loss 48.89426040649414\n",
      "epoch 1018, loss 48.69491958618164\n",
      "epoch 1019, loss 48.496421813964844\n",
      "epoch 1020, loss 48.29869842529297\n",
      "epoch 1021, loss 48.1017951965332\n",
      "epoch 1022, loss 47.905723571777344\n",
      "epoch 1023, loss 47.710487365722656\n",
      "epoch 1024, loss 47.51603317260742\n",
      "epoch 1025, loss 47.32236099243164\n",
      "epoch 1026, loss 47.12950897216797\n",
      "epoch 1027, loss 46.93743896484375\n",
      "epoch 1028, loss 46.74614715576172\n",
      "epoch 1029, loss 46.555667877197266\n",
      "epoch 1030, loss 46.36594772338867\n",
      "epoch 1031, loss 46.1769905090332\n",
      "epoch 1032, loss 45.98881530761719\n",
      "epoch 1033, loss 45.801387786865234\n",
      "epoch 1034, loss 45.614784240722656\n",
      "epoch 1035, loss 45.4289665222168\n",
      "epoch 1036, loss 45.24390411376953\n",
      "epoch 1037, loss 45.05958557128906\n",
      "epoch 1038, loss 44.87603759765625\n",
      "epoch 1039, loss 44.69322204589844\n",
      "epoch 1040, loss 44.51115798950195\n",
      "epoch 1041, loss 44.32982635498047\n",
      "epoch 1042, loss 44.14926528930664\n",
      "epoch 1043, loss 43.96941375732422\n",
      "epoch 1044, loss 43.79032897949219\n",
      "epoch 1045, loss 43.61198425292969\n",
      "epoch 1046, loss 43.43438720703125\n",
      "epoch 1047, loss 43.25749969482422\n",
      "epoch 1048, loss 43.08137130737305\n",
      "epoch 1049, loss 42.905906677246094\n",
      "epoch 1050, loss 42.73119354248047\n",
      "epoch 1051, loss 42.55720138549805\n",
      "epoch 1052, loss 42.38389587402344\n",
      "epoch 1053, loss 42.2113151550293\n",
      "epoch 1054, loss 42.03941345214844\n",
      "epoch 1055, loss 41.86823272705078\n",
      "epoch 1056, loss 41.69776916503906\n",
      "epoch 1057, loss 41.52802276611328\n",
      "epoch 1058, loss 41.35896301269531\n",
      "epoch 1059, loss 41.190589904785156\n",
      "epoch 1060, loss 41.02293014526367\n",
      "epoch 1061, loss 40.855934143066406\n",
      "epoch 1062, loss 40.689640045166016\n",
      "epoch 1063, loss 40.52401351928711\n",
      "epoch 1064, loss 40.359073638916016\n",
      "epoch 1065, loss 40.19483184814453\n",
      "epoch 1066, loss 40.03125762939453\n",
      "epoch 1067, loss 39.868370056152344\n",
      "epoch 1068, loss 39.706138610839844\n",
      "epoch 1069, loss 39.54453659057617\n",
      "epoch 1070, loss 39.38355255126953\n",
      "epoch 1071, loss 39.22328567504883\n",
      "epoch 1072, loss 39.06367492675781\n",
      "epoch 1073, loss 38.904762268066406\n",
      "epoch 1074, loss 38.74646759033203\n",
      "epoch 1075, loss 38.58882141113281\n",
      "epoch 1076, loss 38.43182373046875\n",
      "epoch 1077, loss 38.27544021606445\n",
      "epoch 1078, loss 38.11974334716797\n",
      "epoch 1079, loss 37.964630126953125\n",
      "epoch 1080, loss 37.81019592285156\n",
      "epoch 1081, loss 37.65637969970703\n",
      "epoch 1082, loss 37.50321960449219\n",
      "epoch 1083, loss 37.35068893432617\n",
      "epoch 1084, loss 37.19875717163086\n",
      "epoch 1085, loss 37.047454833984375\n",
      "epoch 1086, loss 36.896785736083984\n",
      "epoch 1087, loss 36.746726989746094\n",
      "epoch 1088, loss 36.59729766845703\n",
      "epoch 1089, loss 36.448524475097656\n",
      "epoch 1090, loss 36.300315856933594\n",
      "epoch 1091, loss 36.15271759033203\n",
      "epoch 1092, loss 36.00572967529297\n",
      "epoch 1093, loss 35.85936737060547\n",
      "epoch 1094, loss 35.71357727050781\n",
      "epoch 1095, loss 35.568382263183594\n",
      "epoch 1096, loss 35.42378234863281\n",
      "epoch 1097, loss 35.27976608276367\n",
      "epoch 1098, loss 35.1363525390625\n",
      "epoch 1099, loss 34.99354934692383\n",
      "epoch 1100, loss 34.85133361816406\n",
      "epoch 1101, loss 34.70973205566406\n",
      "epoch 1102, loss 34.56865692138672\n",
      "epoch 1103, loss 34.428123474121094\n",
      "epoch 1104, loss 34.28822708129883\n",
      "epoch 1105, loss 34.14891052246094\n",
      "epoch 1106, loss 34.01012420654297\n",
      "epoch 1107, loss 33.871925354003906\n",
      "epoch 1108, loss 33.73431396484375\n",
      "epoch 1109, loss 33.59727096557617\n",
      "epoch 1110, loss 33.46076583862305\n",
      "epoch 1111, loss 33.32485580444336\n",
      "epoch 1112, loss 33.18946838378906\n",
      "epoch 1113, loss 33.05464172363281\n",
      "epoch 1114, loss 32.92034912109375\n",
      "epoch 1115, loss 32.78664779663086\n",
      "epoch 1116, loss 32.6534538269043\n",
      "epoch 1117, loss 32.52082824707031\n",
      "epoch 1118, loss 32.38875961303711\n",
      "epoch 1119, loss 32.25719451904297\n",
      "epoch 1120, loss 32.12620544433594\n",
      "epoch 1121, loss 31.99575424194336\n",
      "epoch 1122, loss 31.86583709716797\n",
      "epoch 1123, loss 31.73643684387207\n",
      "epoch 1124, loss 31.607595443725586\n",
      "epoch 1125, loss 31.47927474975586\n",
      "epoch 1126, loss 31.35144805908203\n",
      "epoch 1127, loss 31.22416114807129\n",
      "epoch 1128, loss 31.09739112854004\n",
      "epoch 1129, loss 30.971168518066406\n",
      "epoch 1130, loss 30.845458984375\n",
      "epoch 1131, loss 30.72025489807129\n",
      "epoch 1132, loss 30.595603942871094\n",
      "epoch 1133, loss 30.47144317626953\n",
      "epoch 1134, loss 30.34779930114746\n",
      "epoch 1135, loss 30.22466278076172\n",
      "epoch 1136, loss 30.102027893066406\n",
      "epoch 1137, loss 29.979881286621094\n",
      "epoch 1138, loss 29.85822868347168\n",
      "epoch 1139, loss 29.73709487915039\n",
      "epoch 1140, loss 29.616458892822266\n",
      "epoch 1141, loss 29.49631690979004\n",
      "epoch 1142, loss 29.37668228149414\n",
      "epoch 1143, loss 29.25752830505371\n",
      "epoch 1144, loss 29.13883399963379\n",
      "epoch 1145, loss 29.02066993713379\n",
      "epoch 1146, loss 28.90298080444336\n",
      "epoch 1147, loss 28.78577423095703\n",
      "epoch 1148, loss 28.66904067993164\n",
      "epoch 1149, loss 28.552778244018555\n",
      "epoch 1150, loss 28.4370174407959\n",
      "epoch 1151, loss 28.32172393798828\n",
      "epoch 1152, loss 28.20687484741211\n",
      "epoch 1153, loss 28.092493057250977\n",
      "epoch 1154, loss 27.978605270385742\n",
      "epoch 1155, loss 27.86518669128418\n",
      "epoch 1156, loss 27.75222396850586\n",
      "epoch 1157, loss 27.639732360839844\n",
      "epoch 1158, loss 27.527694702148438\n",
      "epoch 1159, loss 27.416126251220703\n",
      "epoch 1160, loss 27.305021286010742\n",
      "epoch 1161, loss 27.19439697265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1162, loss 27.084218978881836\n",
      "epoch 1163, loss 26.974491119384766\n",
      "epoch 1164, loss 26.865196228027344\n",
      "epoch 1165, loss 26.75634002685547\n",
      "epoch 1166, loss 26.647945404052734\n",
      "epoch 1167, loss 26.53998374938965\n",
      "epoch 1168, loss 26.43245506286621\n",
      "epoch 1169, loss 26.325361251831055\n",
      "epoch 1170, loss 26.21874237060547\n",
      "epoch 1171, loss 26.11252212524414\n",
      "epoch 1172, loss 26.006771087646484\n",
      "epoch 1173, loss 25.901451110839844\n",
      "epoch 1174, loss 25.79656219482422\n",
      "epoch 1175, loss 25.692081451416016\n",
      "epoch 1176, loss 25.588035583496094\n",
      "epoch 1177, loss 25.48440933227539\n",
      "epoch 1178, loss 25.38125228881836\n",
      "epoch 1179, loss 25.278512954711914\n",
      "epoch 1180, loss 25.176204681396484\n",
      "epoch 1181, loss 25.074289321899414\n",
      "epoch 1182, loss 24.972816467285156\n",
      "epoch 1183, loss 24.87177085876465\n",
      "epoch 1184, loss 24.771102905273438\n",
      "epoch 1185, loss 24.670848846435547\n",
      "epoch 1186, loss 24.571033477783203\n",
      "epoch 1187, loss 24.471607208251953\n",
      "epoch 1188, loss 24.37256622314453\n",
      "epoch 1189, loss 24.27393341064453\n",
      "epoch 1190, loss 24.175729751586914\n",
      "epoch 1191, loss 24.077911376953125\n",
      "epoch 1192, loss 23.98048210144043\n",
      "epoch 1193, loss 23.883460998535156\n",
      "epoch 1194, loss 23.786861419677734\n",
      "epoch 1195, loss 23.690664291381836\n",
      "epoch 1196, loss 23.594846725463867\n",
      "epoch 1197, loss 23.499433517456055\n",
      "epoch 1198, loss 23.40439224243164\n",
      "epoch 1199, loss 23.30976104736328\n",
      "epoch 1200, loss 23.215524673461914\n",
      "epoch 1201, loss 23.12169075012207\n",
      "epoch 1202, loss 23.0281982421875\n",
      "epoch 1203, loss 22.935104370117188\n",
      "epoch 1204, loss 22.84239959716797\n",
      "epoch 1205, loss 22.750051498413086\n",
      "epoch 1206, loss 22.65812110900879\n",
      "epoch 1207, loss 22.56655502319336\n",
      "epoch 1208, loss 22.47534942626953\n",
      "epoch 1209, loss 22.384538650512695\n",
      "epoch 1210, loss 22.294084548950195\n",
      "epoch 1211, loss 22.204010009765625\n",
      "epoch 1212, loss 22.11429214477539\n",
      "epoch 1213, loss 22.024938583374023\n",
      "epoch 1214, loss 21.935951232910156\n",
      "epoch 1215, loss 21.847328186035156\n",
      "epoch 1216, loss 21.759084701538086\n",
      "epoch 1217, loss 21.671222686767578\n",
      "epoch 1218, loss 21.583730697631836\n",
      "epoch 1219, loss 21.49657440185547\n",
      "epoch 1220, loss 21.409738540649414\n",
      "epoch 1221, loss 21.323286056518555\n",
      "epoch 1222, loss 21.237192153930664\n",
      "epoch 1223, loss 21.151432037353516\n",
      "epoch 1224, loss 21.06604766845703\n",
      "epoch 1225, loss 20.98103141784668\n",
      "epoch 1226, loss 20.89636993408203\n",
      "epoch 1227, loss 20.81203269958496\n",
      "epoch 1228, loss 20.7280216217041\n",
      "epoch 1229, loss 20.64438247680664\n",
      "epoch 1230, loss 20.56110382080078\n",
      "epoch 1231, loss 20.478168487548828\n",
      "epoch 1232, loss 20.395580291748047\n",
      "epoch 1233, loss 20.31328582763672\n",
      "epoch 1234, loss 20.23131561279297\n",
      "epoch 1235, loss 20.149688720703125\n",
      "epoch 1236, loss 20.068401336669922\n",
      "epoch 1237, loss 19.98746109008789\n",
      "epoch 1238, loss 19.90686798095703\n",
      "epoch 1239, loss 19.826580047607422\n",
      "epoch 1240, loss 19.746620178222656\n",
      "epoch 1241, loss 19.66700553894043\n",
      "epoch 1242, loss 19.587738037109375\n",
      "epoch 1243, loss 19.508745193481445\n",
      "epoch 1244, loss 19.430084228515625\n",
      "epoch 1245, loss 19.351768493652344\n",
      "epoch 1246, loss 19.273765563964844\n",
      "epoch 1247, loss 19.19609832763672\n",
      "epoch 1248, loss 19.11874771118164\n",
      "epoch 1249, loss 19.04170036315918\n",
      "epoch 1250, loss 18.964981079101562\n",
      "epoch 1251, loss 18.888568878173828\n",
      "epoch 1252, loss 18.81248664855957\n",
      "epoch 1253, loss 18.736684799194336\n",
      "epoch 1254, loss 18.661230087280273\n",
      "epoch 1255, loss 18.58609390258789\n",
      "epoch 1256, loss 18.51123809814453\n",
      "epoch 1257, loss 18.43669319152832\n",
      "epoch 1258, loss 18.36246681213379\n",
      "epoch 1259, loss 18.28853416442871\n",
      "epoch 1260, loss 18.21489906311035\n",
      "epoch 1261, loss 18.14159393310547\n",
      "epoch 1262, loss 18.068565368652344\n",
      "epoch 1263, loss 17.995834350585938\n",
      "epoch 1264, loss 17.923402786254883\n",
      "epoch 1265, loss 17.851274490356445\n",
      "epoch 1266, loss 17.77945899963379\n",
      "epoch 1267, loss 17.707923889160156\n",
      "epoch 1268, loss 17.636695861816406\n",
      "epoch 1269, loss 17.565757751464844\n",
      "epoch 1270, loss 17.49506378173828\n",
      "epoch 1271, loss 17.424711227416992\n",
      "epoch 1272, loss 17.35464096069336\n",
      "epoch 1273, loss 17.28484344482422\n",
      "epoch 1274, loss 17.2153377532959\n",
      "epoch 1275, loss 17.146102905273438\n",
      "epoch 1276, loss 17.077186584472656\n",
      "epoch 1277, loss 17.00851821899414\n",
      "epoch 1278, loss 16.940141677856445\n",
      "epoch 1279, loss 16.872074127197266\n",
      "epoch 1280, loss 16.804269790649414\n",
      "epoch 1281, loss 16.73672866821289\n",
      "epoch 1282, loss 16.669490814208984\n",
      "epoch 1283, loss 16.602529525756836\n",
      "epoch 1284, loss 16.535823822021484\n",
      "epoch 1285, loss 16.46938705444336\n",
      "epoch 1286, loss 16.403213500976562\n",
      "epoch 1287, loss 16.33734130859375\n",
      "epoch 1288, loss 16.27172088623047\n",
      "epoch 1289, loss 16.20636749267578\n",
      "epoch 1290, loss 16.141279220581055\n",
      "epoch 1291, loss 16.076480865478516\n",
      "epoch 1292, loss 16.01193618774414\n",
      "epoch 1293, loss 15.947660446166992\n",
      "epoch 1294, loss 15.883649826049805\n",
      "epoch 1295, loss 15.819905281066895\n",
      "epoch 1296, loss 15.75641918182373\n",
      "epoch 1297, loss 15.693195343017578\n",
      "epoch 1298, loss 15.630205154418945\n",
      "epoch 1299, loss 15.56748104095459\n",
      "epoch 1300, loss 15.505016326904297\n",
      "epoch 1301, loss 15.44283676147461\n",
      "epoch 1302, loss 15.38090991973877\n",
      "epoch 1303, loss 15.319242477416992\n",
      "epoch 1304, loss 15.25782585144043\n",
      "epoch 1305, loss 15.196633338928223\n",
      "epoch 1306, loss 15.13571834564209\n",
      "epoch 1307, loss 15.075047492980957\n",
      "epoch 1308, loss 15.014628410339355\n",
      "epoch 1309, loss 14.954448699951172\n",
      "epoch 1310, loss 14.894535064697266\n",
      "epoch 1311, loss 14.83486270904541\n",
      "epoch 1312, loss 14.775434494018555\n",
      "epoch 1313, loss 14.71623706817627\n",
      "epoch 1314, loss 14.657291412353516\n",
      "epoch 1315, loss 14.59858512878418\n",
      "epoch 1316, loss 14.54013442993164\n",
      "epoch 1317, loss 14.481879234313965\n",
      "epoch 1318, loss 14.42388916015625\n",
      "epoch 1319, loss 14.36612606048584\n",
      "epoch 1320, loss 14.308612823486328\n",
      "epoch 1321, loss 14.25131893157959\n",
      "epoch 1322, loss 14.194268226623535\n",
      "epoch 1323, loss 14.137457847595215\n",
      "epoch 1324, loss 14.08088493347168\n",
      "epoch 1325, loss 14.0245361328125\n",
      "epoch 1326, loss 13.968424797058105\n",
      "epoch 1327, loss 13.912529945373535\n",
      "epoch 1328, loss 13.856865882873535\n",
      "epoch 1329, loss 13.801413536071777\n",
      "epoch 1330, loss 13.746199607849121\n",
      "epoch 1331, loss 13.691214561462402\n",
      "epoch 1332, loss 13.636460304260254\n",
      "epoch 1333, loss 13.581932067871094\n",
      "epoch 1334, loss 13.527618408203125\n",
      "epoch 1335, loss 13.473560333251953\n",
      "epoch 1336, loss 13.419697761535645\n",
      "epoch 1337, loss 13.36607837677002\n",
      "epoch 1338, loss 13.312675476074219\n",
      "epoch 1339, loss 13.25948429107666\n",
      "epoch 1340, loss 13.20652961730957\n",
      "epoch 1341, loss 13.15379524230957\n",
      "epoch 1342, loss 13.101253509521484\n",
      "epoch 1343, loss 13.048946380615234\n",
      "epoch 1344, loss 12.996851921081543\n",
      "epoch 1345, loss 12.94497013092041\n",
      "epoch 1346, loss 12.893282890319824\n",
      "epoch 1347, loss 12.841808319091797\n",
      "epoch 1348, loss 12.79053783416748\n",
      "epoch 1349, loss 12.739509582519531\n",
      "epoch 1350, loss 12.688678741455078\n",
      "epoch 1351, loss 12.638038635253906\n",
      "epoch 1352, loss 12.587605476379395\n",
      "epoch 1353, loss 12.537389755249023\n",
      "epoch 1354, loss 12.48737907409668\n",
      "epoch 1355, loss 12.43758773803711\n",
      "epoch 1356, loss 12.387989044189453\n",
      "epoch 1357, loss 12.338614463806152\n",
      "epoch 1358, loss 12.289436340332031\n",
      "epoch 1359, loss 12.240461349487305\n",
      "epoch 1360, loss 12.19167423248291\n",
      "epoch 1361, loss 12.143088340759277\n",
      "epoch 1362, loss 12.094710350036621\n",
      "epoch 1363, loss 12.046540260314941\n",
      "epoch 1364, loss 11.998560905456543\n",
      "epoch 1365, loss 11.950756072998047\n",
      "epoch 1366, loss 11.903156280517578\n",
      "epoch 1367, loss 11.855757713317871\n",
      "epoch 1368, loss 11.8085355758667\n",
      "epoch 1369, loss 11.76150894165039\n",
      "epoch 1370, loss 11.714698791503906\n",
      "epoch 1371, loss 11.66806697845459\n",
      "epoch 1372, loss 11.621634483337402\n",
      "epoch 1373, loss 11.575376510620117\n",
      "epoch 1374, loss 11.529312133789062\n",
      "epoch 1375, loss 11.483450889587402\n",
      "epoch 1376, loss 11.437785148620605\n",
      "epoch 1377, loss 11.392313003540039\n",
      "epoch 1378, loss 11.347003936767578\n",
      "epoch 1379, loss 11.301876068115234\n",
      "epoch 1380, loss 11.256948471069336\n",
      "epoch 1381, loss 11.21219539642334\n",
      "epoch 1382, loss 11.167631149291992\n",
      "epoch 1383, loss 11.123262405395508\n",
      "epoch 1384, loss 11.079059600830078\n",
      "epoch 1385, loss 11.03504467010498\n",
      "epoch 1386, loss 10.991211891174316\n",
      "epoch 1387, loss 10.94756031036377\n",
      "epoch 1388, loss 10.904074668884277\n",
      "epoch 1389, loss 10.860787391662598\n",
      "epoch 1390, loss 10.817659378051758\n",
      "epoch 1391, loss 10.774712562561035\n",
      "epoch 1392, loss 10.731962203979492\n",
      "epoch 1393, loss 10.689356803894043\n",
      "epoch 1394, loss 10.646924018859863\n",
      "epoch 1395, loss 10.604671478271484\n",
      "epoch 1396, loss 10.562599182128906\n",
      "epoch 1397, loss 10.520706176757812\n",
      "epoch 1398, loss 10.478983879089355\n",
      "epoch 1399, loss 10.437443733215332\n",
      "epoch 1400, loss 10.396077156066895\n",
      "epoch 1401, loss 10.354862213134766\n",
      "epoch 1402, loss 10.313809394836426\n",
      "epoch 1403, loss 10.272926330566406\n",
      "epoch 1404, loss 10.232192993164062\n",
      "epoch 1405, loss 10.19162654876709\n",
      "epoch 1406, loss 10.15125846862793\n",
      "epoch 1407, loss 10.111064910888672\n",
      "epoch 1408, loss 10.071036338806152\n",
      "epoch 1409, loss 10.031157493591309\n",
      "epoch 1410, loss 9.991437911987305\n",
      "epoch 1411, loss 9.95188045501709\n",
      "epoch 1412, loss 9.912482261657715\n",
      "epoch 1413, loss 9.873250961303711\n",
      "epoch 1414, loss 9.834181785583496\n",
      "epoch 1415, loss 9.795280456542969\n",
      "epoch 1416, loss 9.756546020507812\n",
      "epoch 1417, loss 9.717954635620117\n",
      "epoch 1418, loss 9.679519653320312\n",
      "epoch 1419, loss 9.64123249053955\n",
      "epoch 1420, loss 9.60312557220459\n",
      "epoch 1421, loss 9.565159797668457\n",
      "epoch 1422, loss 9.527368545532227\n",
      "epoch 1423, loss 9.489716529846191\n",
      "epoch 1424, loss 9.452230453491211\n",
      "epoch 1425, loss 9.414915084838867\n",
      "epoch 1426, loss 9.377748489379883\n",
      "epoch 1427, loss 9.340713500976562\n",
      "epoch 1428, loss 9.303829193115234\n",
      "epoch 1429, loss 9.26711654663086\n",
      "epoch 1430, loss 9.230545043945312\n",
      "epoch 1431, loss 9.194113731384277\n",
      "epoch 1432, loss 9.157843589782715\n",
      "epoch 1433, loss 9.121691703796387\n",
      "epoch 1434, loss 9.085710525512695\n",
      "epoch 1435, loss 9.049887657165527\n",
      "epoch 1436, loss 9.014225006103516\n",
      "epoch 1437, loss 8.978683471679688\n",
      "epoch 1438, loss 8.943283081054688\n",
      "epoch 1439, loss 8.908026695251465\n",
      "epoch 1440, loss 8.872908592224121\n",
      "epoch 1441, loss 8.837946891784668\n",
      "epoch 1442, loss 8.803131103515625\n",
      "epoch 1443, loss 8.768451690673828\n",
      "epoch 1444, loss 8.733912467956543\n",
      "epoch 1445, loss 8.699535369873047\n",
      "epoch 1446, loss 8.665285110473633\n",
      "epoch 1447, loss 8.631174087524414\n",
      "epoch 1448, loss 8.597207069396973\n",
      "epoch 1449, loss 8.563371658325195\n",
      "epoch 1450, loss 8.529683113098145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1451, loss 8.496146202087402\n",
      "epoch 1452, loss 8.46273422241211\n",
      "epoch 1453, loss 8.429447174072266\n",
      "epoch 1454, loss 8.396302223205566\n",
      "epoch 1455, loss 8.363298416137695\n",
      "epoch 1456, loss 8.330434799194336\n",
      "epoch 1457, loss 8.297718048095703\n",
      "epoch 1458, loss 8.265110969543457\n",
      "epoch 1459, loss 8.232651710510254\n",
      "epoch 1460, loss 8.200321197509766\n",
      "epoch 1461, loss 8.168119430541992\n",
      "epoch 1462, loss 8.136064529418945\n",
      "epoch 1463, loss 8.104146957397461\n",
      "epoch 1464, loss 8.07236099243164\n",
      "epoch 1465, loss 8.040680885314941\n",
      "epoch 1466, loss 8.009126663208008\n",
      "epoch 1467, loss 7.977717876434326\n",
      "epoch 1468, loss 7.946443557739258\n",
      "epoch 1469, loss 7.91529655456543\n",
      "epoch 1470, loss 7.884276390075684\n",
      "epoch 1471, loss 7.853381633758545\n",
      "epoch 1472, loss 7.822626113891602\n",
      "epoch 1473, loss 7.791981220245361\n",
      "epoch 1474, loss 7.761453628540039\n",
      "epoch 1475, loss 7.731067180633545\n",
      "epoch 1476, loss 7.700798988342285\n",
      "epoch 1477, loss 7.670658588409424\n",
      "epoch 1478, loss 7.64064884185791\n",
      "epoch 1479, loss 7.6107354164123535\n",
      "epoch 1480, loss 7.580949783325195\n",
      "epoch 1481, loss 7.551302433013916\n",
      "epoch 1482, loss 7.5217742919921875\n",
      "epoch 1483, loss 7.492360591888428\n",
      "epoch 1484, loss 7.46307373046875\n",
      "epoch 1485, loss 7.43391752243042\n",
      "epoch 1486, loss 7.404868125915527\n",
      "epoch 1487, loss 7.375946044921875\n",
      "epoch 1488, loss 7.347123146057129\n",
      "epoch 1489, loss 7.3184380531311035\n",
      "epoch 1490, loss 7.28986930847168\n",
      "epoch 1491, loss 7.261402606964111\n",
      "epoch 1492, loss 7.233071327209473\n",
      "epoch 1493, loss 7.2048492431640625\n",
      "epoch 1494, loss 7.176732540130615\n",
      "epoch 1495, loss 7.148733139038086\n",
      "epoch 1496, loss 7.120850563049316\n",
      "epoch 1497, loss 7.093091011047363\n",
      "epoch 1498, loss 7.065442085266113\n",
      "epoch 1499, loss 7.037908554077148\n",
      "epoch 1500, loss 7.01047945022583\n",
      "epoch 1501, loss 6.983166217803955\n",
      "epoch 1502, loss 6.955970764160156\n",
      "epoch 1503, loss 6.92888069152832\n",
      "epoch 1504, loss 6.901907920837402\n",
      "epoch 1505, loss 6.875038146972656\n",
      "epoch 1506, loss 6.848267555236816\n",
      "epoch 1507, loss 6.821615219116211\n",
      "epoch 1508, loss 6.795079231262207\n",
      "epoch 1509, loss 6.768651008605957\n",
      "epoch 1510, loss 6.742341995239258\n",
      "epoch 1511, loss 6.716126441955566\n",
      "epoch 1512, loss 6.6900200843811035\n",
      "epoch 1513, loss 6.664037704467773\n",
      "epoch 1514, loss 6.638144016265869\n",
      "epoch 1515, loss 6.612357139587402\n",
      "epoch 1516, loss 6.58667516708374\n",
      "epoch 1517, loss 6.561098098754883\n",
      "epoch 1518, loss 6.535634994506836\n",
      "epoch 1519, loss 6.51026725769043\n",
      "epoch 1520, loss 6.485013008117676\n",
      "epoch 1521, loss 6.4598565101623535\n",
      "epoch 1522, loss 6.434814453125\n",
      "epoch 1523, loss 6.409868240356445\n",
      "epoch 1524, loss 6.385031700134277\n",
      "epoch 1525, loss 6.360299110412598\n",
      "epoch 1526, loss 6.335655689239502\n",
      "epoch 1527, loss 6.31112813949585\n",
      "epoch 1528, loss 6.286696910858154\n",
      "epoch 1529, loss 6.262354373931885\n",
      "epoch 1530, loss 6.238127708435059\n",
      "epoch 1531, loss 6.214011192321777\n",
      "epoch 1532, loss 6.189975738525391\n",
      "epoch 1533, loss 6.166036605834961\n",
      "epoch 1534, loss 6.142187118530273\n",
      "epoch 1535, loss 6.118449687957764\n",
      "epoch 1536, loss 6.094801902770996\n",
      "epoch 1537, loss 6.07124662399292\n",
      "epoch 1538, loss 6.047797203063965\n",
      "epoch 1539, loss 6.024433612823486\n",
      "epoch 1540, loss 6.001160621643066\n",
      "epoch 1541, loss 5.978004455566406\n",
      "epoch 1542, loss 5.954940319061279\n",
      "epoch 1543, loss 5.931970596313477\n",
      "epoch 1544, loss 5.909095764160156\n",
      "epoch 1545, loss 5.88630485534668\n",
      "epoch 1546, loss 5.863612651824951\n",
      "epoch 1547, loss 5.841006755828857\n",
      "epoch 1548, loss 5.818498611450195\n",
      "epoch 1549, loss 5.796097755432129\n",
      "epoch 1550, loss 5.7737884521484375\n",
      "epoch 1551, loss 5.751554012298584\n",
      "epoch 1552, loss 5.729413986206055\n",
      "epoch 1553, loss 5.707372665405273\n",
      "epoch 1554, loss 5.685422897338867\n",
      "epoch 1555, loss 5.663565635681152\n",
      "epoch 1556, loss 5.641785144805908\n",
      "epoch 1557, loss 5.620108604431152\n",
      "epoch 1558, loss 5.598526954650879\n",
      "epoch 1559, loss 5.57701301574707\n",
      "epoch 1560, loss 5.555589199066162\n",
      "epoch 1561, loss 5.5342559814453125\n",
      "epoch 1562, loss 5.513010501861572\n",
      "epoch 1563, loss 5.491861343383789\n",
      "epoch 1564, loss 5.470788955688477\n",
      "epoch 1565, loss 5.449803352355957\n",
      "epoch 1566, loss 5.428911209106445\n",
      "epoch 1567, loss 5.4081010818481445\n",
      "epoch 1568, loss 5.38737154006958\n",
      "epoch 1569, loss 5.366730690002441\n",
      "epoch 1570, loss 5.346185684204102\n",
      "epoch 1571, loss 5.3257246017456055\n",
      "epoch 1572, loss 5.305342674255371\n",
      "epoch 1573, loss 5.2850494384765625\n",
      "epoch 1574, loss 5.264832496643066\n",
      "epoch 1575, loss 5.244696617126465\n",
      "epoch 1576, loss 5.224642753601074\n",
      "epoch 1577, loss 5.204672813415527\n",
      "epoch 1578, loss 5.184781551361084\n",
      "epoch 1579, loss 5.164967060089111\n",
      "epoch 1580, loss 5.14525032043457\n",
      "epoch 1581, loss 5.125608444213867\n",
      "epoch 1582, loss 5.106050968170166\n",
      "epoch 1583, loss 5.086565017700195\n",
      "epoch 1584, loss 5.067147254943848\n",
      "epoch 1585, loss 5.047822952270508\n",
      "epoch 1586, loss 5.028581619262695\n",
      "epoch 1587, loss 5.009408950805664\n",
      "epoch 1588, loss 4.990312576293945\n",
      "epoch 1589, loss 4.971296310424805\n",
      "epoch 1590, loss 4.952361106872559\n",
      "epoch 1591, loss 4.933507919311523\n",
      "epoch 1592, loss 4.914730548858643\n",
      "epoch 1593, loss 4.896036624908447\n",
      "epoch 1594, loss 4.877416610717773\n",
      "epoch 1595, loss 4.858859539031982\n",
      "epoch 1596, loss 4.840378284454346\n",
      "epoch 1597, loss 4.821979999542236\n",
      "epoch 1598, loss 4.803650856018066\n",
      "epoch 1599, loss 4.785414695739746\n",
      "epoch 1600, loss 4.7672343254089355\n",
      "epoch 1601, loss 4.749143123626709\n",
      "epoch 1602, loss 4.731125831604004\n",
      "epoch 1603, loss 4.713178634643555\n",
      "epoch 1604, loss 4.695305347442627\n",
      "epoch 1605, loss 4.67750358581543\n",
      "epoch 1606, loss 4.6597795486450195\n",
      "epoch 1607, loss 4.642128944396973\n",
      "epoch 1608, loss 4.624549388885498\n",
      "epoch 1609, loss 4.607044696807861\n",
      "epoch 1610, loss 4.589600563049316\n",
      "epoch 1611, loss 4.572239398956299\n",
      "epoch 1612, loss 4.554952144622803\n",
      "epoch 1613, loss 4.537735939025879\n",
      "epoch 1614, loss 4.520582675933838\n",
      "epoch 1615, loss 4.503503799438477\n",
      "epoch 1616, loss 4.486494064331055\n",
      "epoch 1617, loss 4.469563007354736\n",
      "epoch 1618, loss 4.452694416046143\n",
      "epoch 1619, loss 4.435886859893799\n",
      "epoch 1620, loss 4.419151306152344\n",
      "epoch 1621, loss 4.402488708496094\n",
      "epoch 1622, loss 4.385897159576416\n",
      "epoch 1623, loss 4.3693647384643555\n",
      "epoch 1624, loss 4.352895259857178\n",
      "epoch 1625, loss 4.33650541305542\n",
      "epoch 1626, loss 4.320179462432861\n",
      "epoch 1627, loss 4.303920269012451\n",
      "epoch 1628, loss 4.287728786468506\n",
      "epoch 1629, loss 4.271601676940918\n",
      "epoch 1630, loss 4.255548477172852\n",
      "epoch 1631, loss 4.239542007446289\n",
      "epoch 1632, loss 4.223612308502197\n",
      "epoch 1633, loss 4.2077555656433105\n",
      "epoch 1634, loss 4.191957473754883\n",
      "epoch 1635, loss 4.1762261390686035\n",
      "epoch 1636, loss 4.160558700561523\n",
      "epoch 1637, loss 4.144959449768066\n",
      "epoch 1638, loss 4.129422664642334\n",
      "epoch 1639, loss 4.113950252532959\n",
      "epoch 1640, loss 4.098539352416992\n",
      "epoch 1641, loss 4.0831990242004395\n",
      "epoch 1642, loss 4.067911624908447\n",
      "epoch 1643, loss 4.052684307098389\n",
      "epoch 1644, loss 4.037519931793213\n",
      "epoch 1645, loss 4.022427082061768\n",
      "epoch 1646, loss 4.007401943206787\n",
      "epoch 1647, loss 3.9924328327178955\n",
      "epoch 1648, loss 3.97751784324646\n",
      "epoch 1649, loss 3.962658405303955\n",
      "epoch 1650, loss 3.947861671447754\n",
      "epoch 1651, loss 3.9331343173980713\n",
      "epoch 1652, loss 3.918471336364746\n",
      "epoch 1653, loss 3.9038586616516113\n",
      "epoch 1654, loss 3.889308452606201\n",
      "epoch 1655, loss 3.874810218811035\n",
      "epoch 1656, loss 3.8603732585906982\n",
      "epoch 1657, loss 3.84600830078125\n",
      "epoch 1658, loss 3.83170223236084\n",
      "epoch 1659, loss 3.81744384765625\n",
      "epoch 1660, loss 3.8032498359680176\n",
      "epoch 1661, loss 3.7891111373901367\n",
      "epoch 1662, loss 3.77504563331604\n",
      "epoch 1663, loss 3.7610347270965576\n",
      "epoch 1664, loss 3.747069835662842\n",
      "epoch 1665, loss 3.7331624031066895\n",
      "epoch 1666, loss 3.719318389892578\n",
      "epoch 1667, loss 3.7055273056030273\n",
      "epoch 1668, loss 3.6917872428894043\n",
      "epoch 1669, loss 3.6781094074249268\n",
      "epoch 1670, loss 3.66448712348938\n",
      "epoch 1671, loss 3.6509296894073486\n",
      "epoch 1672, loss 3.6374218463897705\n",
      "epoch 1673, loss 3.6239633560180664\n",
      "epoch 1674, loss 3.61055326461792\n",
      "epoch 1675, loss 3.5972111225128174\n",
      "epoch 1676, loss 3.583918571472168\n",
      "epoch 1677, loss 3.5706896781921387\n",
      "epoch 1678, loss 3.5575180053710938\n",
      "epoch 1679, loss 3.5443966388702393\n",
      "epoch 1680, loss 3.53132963180542\n",
      "epoch 1681, loss 3.5183160305023193\n",
      "epoch 1682, loss 3.505340099334717\n",
      "epoch 1683, loss 3.492427349090576\n",
      "epoch 1684, loss 3.479567050933838\n",
      "epoch 1685, loss 3.4667651653289795\n",
      "epoch 1686, loss 3.4540202617645264\n",
      "epoch 1687, loss 3.4413235187530518\n",
      "epoch 1688, loss 3.428668975830078\n",
      "epoch 1689, loss 3.416069984436035\n",
      "epoch 1690, loss 3.4035189151763916\n",
      "epoch 1691, loss 3.391024351119995\n",
      "epoch 1692, loss 3.378575086593628\n",
      "epoch 1693, loss 3.3661797046661377\n",
      "epoch 1694, loss 3.3538312911987305\n",
      "epoch 1695, loss 3.3415374755859375\n",
      "epoch 1696, loss 3.329306125640869\n",
      "epoch 1697, loss 3.3171098232269287\n",
      "epoch 1698, loss 3.304962158203125\n",
      "epoch 1699, loss 3.2928669452667236\n",
      "epoch 1700, loss 3.2808339595794678\n",
      "epoch 1701, loss 3.268857002258301\n",
      "epoch 1702, loss 3.2569172382354736\n",
      "epoch 1703, loss 3.2450222969055176\n",
      "epoch 1704, loss 3.233177661895752\n",
      "epoch 1705, loss 3.221385955810547\n",
      "epoch 1706, loss 3.2096357345581055\n",
      "epoch 1707, loss 3.1979434490203857\n",
      "epoch 1708, loss 3.186298370361328\n",
      "epoch 1709, loss 3.1746935844421387\n",
      "epoch 1710, loss 3.163142204284668\n",
      "epoch 1711, loss 3.1516363620758057\n",
      "epoch 1712, loss 3.1401736736297607\n",
      "epoch 1713, loss 3.1287543773651123\n",
      "epoch 1714, loss 3.11739182472229\n",
      "epoch 1715, loss 3.1060619354248047\n",
      "epoch 1716, loss 3.094791889190674\n",
      "epoch 1717, loss 3.0835695266723633\n",
      "epoch 1718, loss 3.0723884105682373\n",
      "epoch 1719, loss 3.0612549781799316\n",
      "epoch 1720, loss 3.050171136856079\n",
      "epoch 1721, loss 3.0391244888305664\n",
      "epoch 1722, loss 3.028120994567871\n",
      "epoch 1723, loss 3.017167091369629\n",
      "epoch 1724, loss 3.0062568187713623\n",
      "epoch 1725, loss 2.99540376663208\n",
      "epoch 1726, loss 2.9845893383026123\n",
      "epoch 1727, loss 2.973809003829956\n",
      "epoch 1728, loss 2.9630770683288574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1729, loss 2.952390670776367\n",
      "epoch 1730, loss 2.9417431354522705\n",
      "epoch 1731, loss 2.931140899658203\n",
      "epoch 1732, loss 2.920591354370117\n",
      "epoch 1733, loss 2.9100840091705322\n",
      "epoch 1734, loss 2.8996152877807617\n",
      "epoch 1735, loss 2.889193058013916\n",
      "epoch 1736, loss 2.878817319869995\n",
      "epoch 1737, loss 2.8684778213500977\n",
      "epoch 1738, loss 2.858180046081543\n",
      "epoch 1739, loss 2.847923755645752\n",
      "epoch 1740, loss 2.8377161026000977\n",
      "epoch 1741, loss 2.827544689178467\n",
      "epoch 1742, loss 2.817413806915283\n",
      "epoch 1743, loss 2.8073253631591797\n",
      "epoch 1744, loss 2.7972793579101562\n",
      "epoch 1745, loss 2.7872767448425293\n",
      "epoch 1746, loss 2.7773091793060303\n",
      "epoch 1747, loss 2.7673873901367188\n",
      "epoch 1748, loss 2.7575013637542725\n",
      "epoch 1749, loss 2.74765682220459\n",
      "epoch 1750, loss 2.737852096557617\n",
      "epoch 1751, loss 2.728090524673462\n",
      "epoch 1752, loss 2.7183728218078613\n",
      "epoch 1753, loss 2.7086942195892334\n",
      "epoch 1754, loss 2.6990604400634766\n",
      "epoch 1755, loss 2.689467191696167\n",
      "epoch 1756, loss 2.6799023151397705\n",
      "epoch 1757, loss 2.6703732013702393\n",
      "epoch 1758, loss 2.6608893871307373\n",
      "epoch 1759, loss 2.6514384746551514\n",
      "epoch 1760, loss 2.6420321464538574\n",
      "epoch 1761, loss 2.6326663494110107\n",
      "epoch 1762, loss 2.6233363151550293\n",
      "epoch 1763, loss 2.6140551567077637\n",
      "epoch 1764, loss 2.6048026084899902\n",
      "epoch 1765, loss 2.5955920219421387\n",
      "epoch 1766, loss 2.5864193439483643\n",
      "epoch 1767, loss 2.577282428741455\n",
      "epoch 1768, loss 2.5681936740875244\n",
      "epoch 1769, loss 2.5591228008270264\n",
      "epoch 1770, loss 2.5501019954681396\n",
      "epoch 1771, loss 2.5411131381988525\n",
      "epoch 1772, loss 2.5321645736694336\n",
      "epoch 1773, loss 2.5232481956481934\n",
      "epoch 1774, loss 2.5143673419952393\n",
      "epoch 1775, loss 2.5055246353149414\n",
      "epoch 1776, loss 2.49672269821167\n",
      "epoch 1777, loss 2.487964153289795\n",
      "epoch 1778, loss 2.479224443435669\n",
      "epoch 1779, loss 2.470527410507202\n",
      "epoch 1780, loss 2.4618613719940186\n",
      "epoch 1781, loss 2.453232526779175\n",
      "epoch 1782, loss 2.4446372985839844\n",
      "epoch 1783, loss 2.43607759475708\n",
      "epoch 1784, loss 2.427556276321411\n",
      "epoch 1785, loss 2.4190778732299805\n",
      "epoch 1786, loss 2.4106216430664062\n",
      "epoch 1787, loss 2.4022021293640137\n",
      "epoch 1788, loss 2.393824815750122\n",
      "epoch 1789, loss 2.3854758739471436\n",
      "epoch 1790, loss 2.377168893814087\n",
      "epoch 1791, loss 2.3688888549804688\n",
      "epoch 1792, loss 2.360651969909668\n",
      "epoch 1793, loss 2.3524441719055176\n",
      "epoch 1794, loss 2.3442633152008057\n",
      "epoch 1795, loss 2.33611798286438\n",
      "epoch 1796, loss 2.328007936477661\n",
      "epoch 1797, loss 2.3199386596679688\n",
      "epoch 1798, loss 2.3119044303894043\n",
      "epoch 1799, loss 2.3038971424102783\n",
      "epoch 1800, loss 2.295919179916382\n",
      "epoch 1801, loss 2.2879817485809326\n",
      "epoch 1802, loss 2.280064582824707\n",
      "epoch 1803, loss 2.272183418273926\n",
      "epoch 1804, loss 2.264340400695801\n",
      "epoch 1805, loss 2.256531238555908\n",
      "epoch 1806, loss 2.2487504482269287\n",
      "epoch 1807, loss 2.2410011291503906\n",
      "epoch 1808, loss 2.233274221420288\n",
      "epoch 1809, loss 2.225584030151367\n",
      "epoch 1810, loss 2.2179269790649414\n",
      "epoch 1811, loss 2.21030330657959\n",
      "epoch 1812, loss 2.2027082443237305\n",
      "epoch 1813, loss 2.1951522827148438\n",
      "epoch 1814, loss 2.187627077102661\n",
      "epoch 1815, loss 2.1801254749298096\n",
      "epoch 1816, loss 2.1726462841033936\n",
      "epoch 1817, loss 2.1652042865753174\n",
      "epoch 1818, loss 2.15779447555542\n",
      "epoch 1819, loss 2.1504180431365967\n",
      "epoch 1820, loss 2.1430675983428955\n",
      "epoch 1821, loss 2.135754108428955\n",
      "epoch 1822, loss 2.128462553024292\n",
      "epoch 1823, loss 2.1212048530578613\n",
      "epoch 1824, loss 2.1139824390411377\n",
      "epoch 1825, loss 2.106778144836426\n",
      "epoch 1826, loss 2.0996100902557373\n",
      "epoch 1827, loss 2.0924737453460693\n",
      "epoch 1828, loss 2.085360527038574\n",
      "epoch 1829, loss 2.078279972076416\n",
      "epoch 1830, loss 2.071233034133911\n",
      "epoch 1831, loss 2.064206838607788\n",
      "epoch 1832, loss 2.057210683822632\n",
      "epoch 1833, loss 2.0502405166625977\n",
      "epoch 1834, loss 2.04331111907959\n",
      "epoch 1835, loss 2.036406993865967\n",
      "epoch 1836, loss 2.029529094696045\n",
      "epoch 1837, loss 2.022678852081299\n",
      "epoch 1838, loss 2.0158486366271973\n",
      "epoch 1839, loss 2.0090560913085938\n",
      "epoch 1840, loss 2.002291679382324\n",
      "epoch 1841, loss 1.995553731918335\n",
      "epoch 1842, loss 1.988837718963623\n",
      "epoch 1843, loss 1.9821505546569824\n",
      "epoch 1844, loss 1.9754865169525146\n",
      "epoch 1845, loss 1.9688568115234375\n",
      "epoch 1846, loss 1.9622505903244019\n",
      "epoch 1847, loss 1.9556753635406494\n",
      "epoch 1848, loss 1.94912850856781\n",
      "epoch 1849, loss 1.942610263824463\n",
      "epoch 1850, loss 1.9361228942871094\n",
      "epoch 1851, loss 1.9296574592590332\n",
      "epoch 1852, loss 1.923216462135315\n",
      "epoch 1853, loss 1.9168052673339844\n",
      "epoch 1854, loss 1.910412311553955\n",
      "epoch 1855, loss 1.9040554761886597\n",
      "epoch 1856, loss 1.8977184295654297\n",
      "epoch 1857, loss 1.8914148807525635\n",
      "epoch 1858, loss 1.8851228952407837\n",
      "epoch 1859, loss 1.878861904144287\n",
      "epoch 1860, loss 1.8726283311843872\n",
      "epoch 1861, loss 1.866422414779663\n",
      "epoch 1862, loss 1.860242247581482\n",
      "epoch 1863, loss 1.8540916442871094\n",
      "epoch 1864, loss 1.847969889640808\n",
      "epoch 1865, loss 1.841867446899414\n",
      "epoch 1866, loss 1.8357911109924316\n",
      "epoch 1867, loss 1.8297407627105713\n",
      "epoch 1868, loss 1.8237078189849854\n",
      "epoch 1869, loss 1.8176989555358887\n",
      "epoch 1870, loss 1.8117209672927856\n",
      "epoch 1871, loss 1.8057657480239868\n",
      "epoch 1872, loss 1.7998403310775757\n",
      "epoch 1873, loss 1.7939363718032837\n",
      "epoch 1874, loss 1.788053274154663\n",
      "epoch 1875, loss 1.7821930646896362\n",
      "epoch 1876, loss 1.7763612270355225\n",
      "epoch 1877, loss 1.770553469657898\n",
      "epoch 1878, loss 1.7647610902786255\n",
      "epoch 1879, loss 1.758995532989502\n",
      "epoch 1880, loss 1.753257155418396\n",
      "epoch 1881, loss 1.7475420236587524\n",
      "epoch 1882, loss 1.7418540716171265\n",
      "epoch 1883, loss 1.7361912727355957\n",
      "epoch 1884, loss 1.7305431365966797\n",
      "epoch 1885, loss 1.7249196767807007\n",
      "epoch 1886, loss 1.7193245887756348\n",
      "epoch 1887, loss 1.7137432098388672\n",
      "epoch 1888, loss 1.7081952095031738\n",
      "epoch 1889, loss 1.7026629447937012\n",
      "epoch 1890, loss 1.6971566677093506\n",
      "epoch 1891, loss 1.6916732788085938\n",
      "epoch 1892, loss 1.686204433441162\n",
      "epoch 1893, loss 1.6807665824890137\n",
      "epoch 1894, loss 1.6753408908843994\n",
      "epoch 1895, loss 1.6699458360671997\n",
      "epoch 1896, loss 1.6645698547363281\n",
      "epoch 1897, loss 1.6592128276824951\n",
      "epoch 1898, loss 1.6538820266723633\n",
      "epoch 1899, loss 1.6485682725906372\n",
      "epoch 1900, loss 1.6432775259017944\n",
      "epoch 1901, loss 1.6380115747451782\n",
      "epoch 1902, loss 1.6327662467956543\n",
      "epoch 1903, loss 1.6275408267974854\n",
      "epoch 1904, loss 1.6223394870758057\n",
      "epoch 1905, loss 1.6171663999557495\n",
      "epoch 1906, loss 1.6120115518569946\n",
      "epoch 1907, loss 1.606878399848938\n",
      "epoch 1908, loss 1.6017664670944214\n",
      "epoch 1909, loss 1.5966739654541016\n",
      "epoch 1910, loss 1.59159517288208\n",
      "epoch 1911, loss 1.5865412950515747\n",
      "epoch 1912, loss 1.5815112590789795\n",
      "epoch 1913, loss 1.5764994621276855\n",
      "epoch 1914, loss 1.5715079307556152\n",
      "epoch 1915, loss 1.5665340423583984\n",
      "epoch 1916, loss 1.5615891218185425\n",
      "epoch 1917, loss 1.5566627979278564\n",
      "epoch 1918, loss 1.5517528057098389\n",
      "epoch 1919, loss 1.5468658208847046\n",
      "epoch 1920, loss 1.5419973134994507\n",
      "epoch 1921, loss 1.5371534824371338\n",
      "epoch 1922, loss 1.5323246717453003\n",
      "epoch 1923, loss 1.5275177955627441\n",
      "epoch 1924, loss 1.5227330923080444\n",
      "epoch 1925, loss 1.5179636478424072\n",
      "epoch 1926, loss 1.5132107734680176\n",
      "epoch 1927, loss 1.508481740951538\n",
      "epoch 1928, loss 1.5037705898284912\n",
      "epoch 1929, loss 1.499077320098877\n",
      "epoch 1930, loss 1.494405746459961\n",
      "epoch 1931, loss 1.4897499084472656\n",
      "epoch 1932, loss 1.4851139783859253\n",
      "epoch 1933, loss 1.480496883392334\n",
      "epoch 1934, loss 1.4758987426757812\n",
      "epoch 1935, loss 1.4713186025619507\n",
      "epoch 1936, loss 1.4667630195617676\n",
      "epoch 1937, loss 1.4622193574905396\n",
      "epoch 1938, loss 1.4576936960220337\n",
      "epoch 1939, loss 1.4531890153884888\n",
      "epoch 1940, loss 1.4487031698226929\n",
      "epoch 1941, loss 1.4442322254180908\n",
      "epoch 1942, loss 1.4397857189178467\n",
      "epoch 1943, loss 1.4353551864624023\n",
      "epoch 1944, loss 1.4309489727020264\n",
      "epoch 1945, loss 1.42655611038208\n",
      "epoch 1946, loss 1.422182559967041\n",
      "epoch 1947, loss 1.4178178310394287\n",
      "epoch 1948, loss 1.4134751558303833\n",
      "epoch 1949, loss 1.409149408340454\n",
      "epoch 1950, loss 1.4048422574996948\n",
      "epoch 1951, loss 1.4005550146102905\n",
      "epoch 1952, loss 1.3962838649749756\n",
      "epoch 1953, loss 1.3920321464538574\n",
      "epoch 1954, loss 1.3878002166748047\n",
      "epoch 1955, loss 1.3835835456848145\n",
      "epoch 1956, loss 1.3793892860412598\n",
      "epoch 1957, loss 1.3752093315124512\n",
      "epoch 1958, loss 1.371042013168335\n",
      "epoch 1959, loss 1.366899013519287\n",
      "epoch 1960, loss 1.362766981124878\n",
      "epoch 1961, loss 1.3586516380310059\n",
      "epoch 1962, loss 1.3545544147491455\n",
      "epoch 1963, loss 1.350476622581482\n",
      "epoch 1964, loss 1.3464131355285645\n",
      "epoch 1965, loss 1.342362880706787\n",
      "epoch 1966, loss 1.338333249092102\n",
      "epoch 1967, loss 1.334320306777954\n",
      "epoch 1968, loss 1.3303226232528687\n",
      "epoch 1969, loss 1.3263397216796875\n",
      "epoch 1970, loss 1.3223795890808105\n",
      "epoch 1971, loss 1.318426251411438\n",
      "epoch 1972, loss 1.3144934177398682\n",
      "epoch 1973, loss 1.310577392578125\n",
      "epoch 1974, loss 1.3066802024841309\n",
      "epoch 1975, loss 1.3027962446212769\n",
      "epoch 1976, loss 1.2989310026168823\n",
      "epoch 1977, loss 1.295084834098816\n",
      "epoch 1978, loss 1.2912497520446777\n",
      "epoch 1979, loss 1.2874330282211304\n",
      "epoch 1980, loss 1.2836312055587769\n",
      "epoch 1981, loss 1.2798396348953247\n",
      "epoch 1982, loss 1.2760701179504395\n",
      "epoch 1983, loss 1.2723133563995361\n",
      "epoch 1984, loss 1.268568754196167\n",
      "epoch 1985, loss 1.2648457288742065\n",
      "epoch 1986, loss 1.2611349821090698\n",
      "epoch 1987, loss 1.2574381828308105\n",
      "epoch 1988, loss 1.2537546157836914\n",
      "epoch 1989, loss 1.250085711479187\n",
      "epoch 1990, loss 1.2464346885681152\n",
      "epoch 1991, loss 1.2428003549575806\n",
      "epoch 1992, loss 1.2391806840896606\n",
      "epoch 1993, loss 1.2355812788009644\n",
      "epoch 1994, loss 1.2319930791854858\n",
      "epoch 1995, loss 1.2284170389175415\n",
      "epoch 1996, loss 1.2248607873916626\n",
      "epoch 1997, loss 1.2213196754455566\n",
      "epoch 1998, loss 1.2177919149398804\n",
      "epoch 1999, loss 1.2142785787582397\n",
      "epoch 2000, loss 1.2107787132263184\n"
     ]
    }
   ],
   "source": [
    "# This code is exactly like the previous one, but includes the changes necessary to run the code on the GPU\n",
    "# Only 2 things need to be in the GPU, your models and your variables.\n",
    "\n",
    "\n",
    "###################################################################\n",
    "#  Import Libraries\n",
    "###################################################################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "###################################################################\n",
    "#  Get and Organize the data\n",
    "###################################################################\n",
    "# We are trying a linear model here. So data should be a column\n",
    "# x_train should have as many rows as y_correct\n",
    "# x_train can have multiple colunmns. but in this case the model must\n",
    "# have input_dim equal to the number of columns\n",
    "\n",
    "# Define Data 1 \n",
    "# This is data entered manually to shape scalars as a numpy array column\n",
    "x_train = np.asarray([[1],[2],[1],[1],[4],[3]],dtype = np.uint8)\n",
    "y_correct =  np.asarray([[3],[6],[6],[4],[12],[9]],dtype = np.uint8)\n",
    "\n",
    "# Define Data 2\n",
    "# alternatively, you can make the process more automatic\n",
    "domain_size = 18\n",
    "x_train = np.arange(domain_size,dtype=np.float64)\n",
    "x_train = np.reshape(x_train, (domain_size,1))\n",
    "error =  (np.random.rand(x_train.shape[0],x_train.shape[1])-0.5) *2\n",
    "bias = 10\n",
    "y_correct = x_train * 5  + error + bias\n",
    "\n",
    "\n",
    "###################################################################\n",
    "# 1- Define the model\n",
    "###################################################################\n",
    "class MyLinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        nn.Module is just an other name for network or module\n",
    "        in __init__() you define the layers of the network\n",
    "        you should specify them with the number of inputs and outputs\n",
    "        nn.Linear for linear layers\n",
    "        \"\"\"\n",
    "        super().__init__() # Calling Super Class's constructor\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        # nn.linear is defined in nn.Module\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Here the forward pass is simply a linear function\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "###################################################################\n",
    "#  TRAINING\n",
    "###################################################################\n",
    "# 2- Instantiate the model class\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "model = MyLinearRegressionModel(input_dim,output_dim)\n",
    "\n",
    "# This code sends your model to run on the GPU\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# 3- Instantiate the Loss class \n",
    "criterion = nn.MSELoss()# Establish loss function: Mean Squared Loss\n",
    "\n",
    "# 4- Instantiate the Optimizer class\n",
    "l_rate = 0.01 # learning rate\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr = l_rate) # Optimization rule: Stochastic Gradient Descent\n",
    "\n",
    "\n",
    "# 5- Train the Model \n",
    "epochs = 2000\n",
    "for epoch in range(epochs):\n",
    "    epoch +=1 #increase the number of epochs by 1 every time\n",
    "    \n",
    "    #Convert numpy arrays to torch variables\n",
    "    if torch.cuda.is_available(): # TO RUN IN GPU\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_correct).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_correct))\n",
    "\n",
    "    # Clears grads from previous epochs. Otherwise they will be accumulated\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Runs forward to get predicted values\n",
    "    outputs = model.forward(inputs.float())\n",
    "    \n",
    "    # This calculates the loss (or error)\n",
    "    loss = criterion(outputs, labels.float()) \n",
    "    \n",
    "    # This getts the gradients with respect to the parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # This updates the parameters\n",
    "    optimiser.step()\n",
    "    \n",
    "    print('epoch {}, loss {}'.format(epoch,loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CUDA but got backend CPU for argument #4 'mat1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8a6b82c3a1ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Show real data on top of the prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m###################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_correct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'go'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'from data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'prediction'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.75\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-fb7dd0c2af08>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m# Here the forward pass is simply a linear function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1352\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1353\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of backend CUDA but got backend CPU for argument #4 'mat1'"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "# Show real data on top of the prediction\n",
    "###################################################################\n",
    "predicted = model.forward(Variable(torch.from_numpy(x_train)).float()).data.numpy()\n",
    "plt.plot(x_train, y_correct, 'go', label = 'from data', alpha = 0.25)\n",
    "plt.plot(x_train, predicted, label = 'prediction', alpha = 0.75)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[5.0022]])), ('linear.bias', tensor([0.9746]))])\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "# How to save a trained model\n",
    "###################################################################\n",
    "save_model = False\n",
    "if save_model is True:\n",
    "    # Saves only parameters\n",
    "    # alpha & beta\n",
    "    torch.save(model.state_dict(), 'awesome_linear_model.pkl')\n",
    "    \n",
    "# Shows the content of what will be saved\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# How to load a trained model\n",
    "###################################################################\n",
    "load_model = True\n",
    "if load_model is True:\n",
    "    model.load_state_dict(torch.load('awesome_linear_model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyLinearRegressionModel(\n",
      "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]\n",
      " [13]\n",
      " [14]\n",
      " [15]\n",
      " [16]\n",
      " [17]\n",
      " [18]\n",
      " [19]\n",
      " [20]\n",
      " [21]\n",
      " [22]\n",
      " [23]\n",
      " [24]\n",
      " [25]\n",
      " [26]\n",
      " [27]\n",
      " [28]\n",
      " [29]\n",
      " [30]\n",
      " [31]\n",
      " [32]\n",
      " [33]\n",
      " [34]\n",
      " [35]\n",
      " [36]\n",
      " [37]\n",
      " [38]\n",
      " [39]\n",
      " [40]\n",
      " [41]\n",
      " [42]\n",
      " [43]\n",
      " [44]\n",
      " [45]\n",
      " [46]\n",
      " [47]\n",
      " [48]\n",
      " [49]\n",
      " [50]\n",
      " [51]\n",
      " [52]\n",
      " [53]\n",
      " [54]\n",
      " [55]\n",
      " [56]\n",
      " [57]\n",
      " [58]\n",
      " [59]\n",
      " [60]\n",
      " [61]\n",
      " [62]\n",
      " [63]\n",
      " [64]\n",
      " [65]\n",
      " [66]\n",
      " [67]\n",
      " [68]\n",
      " [69]\n",
      " [70]\n",
      " [71]\n",
      " [72]\n",
      " [73]\n",
      " [74]\n",
      " [75]\n",
      " [76]\n",
      " [77]\n",
      " [78]\n",
      " [79]\n",
      " [80]\n",
      " [81]\n",
      " [82]\n",
      " [83]\n",
      " [84]\n",
      " [85]\n",
      " [86]\n",
      " [87]\n",
      " [88]\n",
      " [89]\n",
      " [90]\n",
      " [91]\n",
      " [92]\n",
      " [93]\n",
      " [94]\n",
      " [95]\n",
      " [96]\n",
      " [97]\n",
      " [98]\n",
      " [99]] [[ 32.87444537]\n",
      " [ 56.65619381]\n",
      " [ 45.1761277 ]\n",
      " [ 51.60662221]\n",
      " [ 53.27109126]\n",
      " [ 61.97156352]\n",
      " [ 74.29956315]\n",
      " [ 79.15311136]\n",
      " [ 90.31687479]\n",
      " [ 75.59716878]\n",
      " [ 96.12692407]\n",
      " [116.24652448]\n",
      " [107.63529304]\n",
      " [105.2607028 ]\n",
      " [137.70959195]\n",
      " [119.29299919]\n",
      " [113.85796882]\n",
      " [114.85557761]\n",
      " [129.44868392]\n",
      " [128.80124315]\n",
      " [132.94086512]\n",
      " [134.94147609]\n",
      " [147.35700861]\n",
      " [144.29337895]\n",
      " [172.64184253]\n",
      " [157.63038827]\n",
      " [161.18188834]\n",
      " [162.75877884]\n",
      " [178.85783724]\n",
      " [181.64524968]\n",
      " [195.4494961 ]\n",
      " [158.60442531]\n",
      " [198.59576815]\n",
      " [212.42627286]\n",
      " [176.85118414]\n",
      " [222.93696473]\n",
      " [222.03656568]\n",
      " [193.59270822]\n",
      " [245.27191765]\n",
      " [188.73310868]\n",
      " [208.92705711]\n",
      " [250.52117386]\n",
      " [237.04960339]\n",
      " [252.99819731]\n",
      " [249.56041351]\n",
      " [254.53210265]\n",
      " [267.64219994]\n",
      " [264.92032808]\n",
      " [285.29707639]\n",
      " [276.82659152]\n",
      " [300.49160614]\n",
      " [289.84249338]\n",
      " [286.47275882]\n",
      " [308.02981612]\n",
      " [286.10483836]\n",
      " [295.9138107 ]\n",
      " [320.08336414]\n",
      " [303.86330908]\n",
      " [320.16097073]\n",
      " [326.28213468]\n",
      " [328.45092535]\n",
      " [339.57985686]\n",
      " [333.10915013]\n",
      " [348.56568623]\n",
      " [352.10016758]\n",
      " [343.847371  ]\n",
      " [347.40802225]\n",
      " [355.2761888 ]\n",
      " [378.30096032]\n",
      " [361.76257193]\n",
      " [383.22671179]\n",
      " [388.505594  ]\n",
      " [380.94303578]\n",
      " [390.89403971]\n",
      " [394.38112885]\n",
      " [395.06223618]\n",
      " [417.51751092]\n",
      " [396.01308512]\n",
      " [401.02874617]\n",
      " [411.89500802]\n",
      " [435.1503714 ]\n",
      " [436.10774862]\n",
      " [429.09542814]\n",
      " [428.09180864]\n",
      " [456.5089308 ]\n",
      " [435.53392004]\n",
      " [456.66518481]\n",
      " [453.58618657]\n",
      " [463.77857836]\n",
      " [461.52321517]\n",
      " [446.0489458 ]\n",
      " [462.20483675]\n",
      " [486.69540883]\n",
      " [480.75236063]\n",
      " [507.28068577]\n",
      " [499.03289737]\n",
      " [511.60054973]\n",
      " [522.15014223]\n",
      " [500.65261223]]\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "# How to use the model on new data\n",
    "###################################################################\n",
    "print(model.eval()) # Shows the model and describes its parameters\n",
    "\n",
    "#Load data to be predicted\n",
    "x = np.reshape(np.arange(1,100),(np.arange(1,100).shape[0],1))\n",
    "data = Variable(torch.from_numpy(x)).float()\n",
    "\n",
    "# Calculate the prediction\n",
    "#output = model.forward(data)\n",
    "#prediction = torch.argmax(output)\n",
    "#predicted = prediction.data.numpy()\n",
    "\n",
    "# Calculate the prediction\n",
    "predicted = model.forward(Variable(torch.from_numpy(x)).float()).data.numpy()\n",
    "\n",
    "\n",
    "# \"real data\"\n",
    "y = x*4.68 +41 + np.random.randn(np.arange(1,100).shape[0],1)*12\n",
    "\n",
    "# Visualize the data\n",
    "plt.plot(x, y, 'go', label = 'from data', alpha = 0.25, color='r')\n",
    "plt.plot(x, predicted, label = 'prediction', alpha = 0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 46.2167],\n",
      "        [ 50.9013],\n",
      "        [ 55.5860],\n",
      "        [ 60.2706],\n",
      "        [ 64.9553],\n",
      "        [ 69.6399],\n",
      "        [ 74.3246],\n",
      "        [ 79.0092],\n",
      "        [ 83.6939],\n",
      "        [ 88.3785],\n",
      "        [ 93.0632],\n",
      "        [ 97.7478],\n",
      "        [102.4325],\n",
      "        [107.1171],\n",
      "        [111.8018],\n",
      "        [116.4864],\n",
      "        [121.1711],\n",
      "        [125.8557],\n",
      "        [130.5404],\n",
      "        [135.2250],\n",
      "        [139.9097],\n",
      "        [144.5943],\n",
      "        [149.2790],\n",
      "        [153.9637],\n",
      "        [158.6483],\n",
      "        [163.3329],\n",
      "        [168.0176],\n",
      "        [172.7022],\n",
      "        [177.3869],\n",
      "        [182.0715],\n",
      "        [186.7562],\n",
      "        [191.4409],\n",
      "        [196.1255],\n",
      "        [200.8102],\n",
      "        [205.4948],\n",
      "        [210.1794],\n",
      "        [214.8641],\n",
      "        [219.5488],\n",
      "        [224.2334],\n",
      "        [228.9181],\n",
      "        [233.6027],\n",
      "        [238.2874],\n",
      "        [242.9720],\n",
      "        [247.6566],\n",
      "        [252.3413],\n",
      "        [257.0260],\n",
      "        [261.7106],\n",
      "        [266.3953],\n",
      "        [271.0799],\n",
      "        [275.7646],\n",
      "        [280.4492],\n",
      "        [285.1339],\n",
      "        [289.8185],\n",
      "        [294.5032],\n",
      "        [299.1878],\n",
      "        [303.8725],\n",
      "        [308.5571],\n",
      "        [313.2418],\n",
      "        [317.9264],\n",
      "        [322.6111],\n",
      "        [327.2957],\n",
      "        [331.9804],\n",
      "        [336.6650],\n",
      "        [341.3497],\n",
      "        [346.0343],\n",
      "        [350.7190],\n",
      "        [355.4036],\n",
      "        [360.0883],\n",
      "        [364.7729],\n",
      "        [369.4576],\n",
      "        [374.1422],\n",
      "        [378.8269],\n",
      "        [383.5115],\n",
      "        [388.1962],\n",
      "        [392.8808],\n",
      "        [397.5655],\n",
      "        [402.2501],\n",
      "        [406.9348],\n",
      "        [411.6194],\n",
      "        [416.3041],\n",
      "        [420.9887],\n",
      "        [425.6734],\n",
      "        [430.3580],\n",
      "        [435.0427],\n",
      "        [439.7273],\n",
      "        [444.4120],\n",
      "        [449.0966],\n",
      "        [453.7813],\n",
      "        [458.4659],\n",
      "        [463.1506],\n",
      "        [467.8352],\n",
      "        [472.5199],\n",
      "        [477.2046],\n",
      "        [481.8892],\n",
      "        [486.5739],\n",
      "        [491.2585],\n",
      "        [495.9431],\n",
      "        [500.6278],\n",
      "        [505.3124]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "predicted = model.forward(Variable(torch.from_numpy(x)).float()).data.numpy()\n",
    "plt.plot(x, predicted, label = 'prediction', alpha = 0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.forward(Variable(torch.from_numpy(x)).float()).data.numpy()\n",
    "?torch.argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 99 into shape (98,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-1c512079744e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    277\u001b[0m            [5, 6]])\n\u001b[0;32m    278\u001b[0m     \"\"\"\n\u001b[1;32m--> 279\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reshape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 99 into shape (98,1)"
     ]
    }
   ],
   "source": [
    "print(np.reshape(np.arange(1,100),(len(np.arange(1,100))-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c3d8c93f720f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
